{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Lab Session Week 8\n",
    "### Constructing Feature Sets for Sentiment Classification in the NLTK\n",
    "### Part 1:  Movie Review Corpus Sentences with BOW\n",
    "\n",
    "#### Getting Started\n",
    "\n",
    "For this lab session download the following files and put them in your class folder for copy/pasting examples.  \n",
    "\n",
    "LabWk8.sentimentfeatures.sents.txt\n",
    "\n",
    "Subjectivity.py\n",
    "\n",
    "subjclueslen1-HLTEMNLP05.tff.zip\n",
    "\n",
    "Unzip the subjclues file and remember the location.  Start your jupyter notebook session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment/Opinion Classification (using the Movie Review corpus sentences)\n",
    "\n",
    "In today’s lab, we will look at two ways to add features that are sometimes used in various sentiment or opinion classification problems.  In addition to providing a corpus of the 2000 positive and negative movie review documents, Pang and Lee had a subset of the sentences of the corpus annotated for sentiment in each sentence.  We will illustrate the process of sentiment classification on this corpus of sentences with positive or negative sentiment labels.\n",
    "\n",
    "We start by loading the sentence_polarity corpus and creating a list of documents where each document represents a single sentence with the words and its label. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import sentence_polarity\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10662\n",
      "<class 'nltk.corpus.reader.util.ConcatenatedCorpusView'>\n",
      "['neg', 'pos']\n",
      "['simplistic', ',', 'silly', 'and', 'tedious', '.']\n",
      "[\"it's\", 'so', 'laddish', 'and', 'juvenile', ',', 'only', 'teenage', 'boys', 'could', 'possibly', 'find', 'it', 'funny', '.']\n",
      "['exploitative', 'and', 'largely', 'devoid', 'of', 'the', 'depth', 'or', 'sophistication', 'that', 'would', 'make', 'watching', 'such', 'a', 'graphic', 'treatment', 'of', 'the', 'crimes', 'bearable', '.']\n",
      "['[garbus]', 'discards', 'the', 'potential', 'for', 'pathological', 'study', ',', 'exhuming', 'instead', ',', 'the', 'skewed', 'melodrama', 'of', 'the', 'circumstantial', 'situation', '.']\n"
     ]
    }
   ],
   "source": [
    "# Look at sentences from the entire list of sentences.\n",
    "sentences = sentence_polarity.sents()\n",
    "print(len(sentences))\n",
    "print(type(sentences))\n",
    "print(sentence_polarity.categories())\n",
    "# sentences are already tokenized, show the first four sentences\n",
    "for sent in sentences[:4]:\n",
    "    print(sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The movie review sentences are not labeled individually, but can be retrieved by category.  Look at the sentences by category to see how many positive and negative sentences there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5331\n",
      "5331\n"
     ]
    }
   ],
   "source": [
    "pos_sents = sentence_polarity.sents(categories='pos')\n",
    "print(len(pos_sents))\n",
    "neg_sents = sentence_polarity.sents(categories='neg')\n",
    "print(len(neg_sents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create the list of documents where each document(sentence) is paired with its label.\n",
    "\n",
    "documents = [(sent, cat) for cat in sentence_polarity.categories() \n",
    "\tfor sent in sentence_polarity.sents(categories=cat)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['simplistic', ',', 'silly', 'and', 'tedious', '.'], 'neg')\n",
      "(['provides', 'a', 'porthole', 'into', 'that', 'noble', ',', 'trembling', 'incoherence', 'that', 'defines', 'us', 'all', '.'], 'pos')\n"
     ]
    }
   ],
   "source": [
    "#In this list, each item is a pair (sent,cat) where sent is a list of words from a movie review sentence and cat is its label, either ‘pos’ or ‘neg’.\n",
    "print(documents[0])\n",
    "print(documents[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the documents are in order by label, we mix them up for later separation into training and test sets.\n",
    "\n",
    "random.shuffle(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the set of words that will be used for features.  This is essentially all the words in the entire document collection, except that we will limit it to the 2000 most frequent words.  Note that we lowercase the words, but do not do stemming or remove stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'the', ',', 'a', 'and', 'of', 'to', 'is', 'in', 'that', 'it', 'as', 'but', 'with', 'film', 'this', 'for', 'its', 'an', 'movie', \"it's\", 'be', 'on', 'you', 'not', 'by', 'about', 'one', 'more', 'like', 'has', 'are', 'at', 'from', 'than', '\"', 'all', '--', 'his', 'have', 'so', 'if', 'or', 'story', 'i', 'too', 'just', 'who', 'into', 'what']\n"
     ]
    }
   ],
   "source": [
    "all_words_list = [word for (sent,cat) in documents for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "word_items = all_words.most_common(2000)\n",
    "word_features = [word for (word, freq) in word_items]\n",
    "# look at the first 50 words in the most frequent list of words\n",
    "print(word_features[:50])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the features for each document, using just the words, sometimes called the BOW or unigram features.  The feature label will be ‘V_keyword’ for each keyword (aka word) in the word_features set, and the value of the feature will be Boolean, according to whether the word is contained in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['not',\n",
       "  'only',\n",
       "  'a',\n",
       "  'reminder',\n",
       "  'of',\n",
       "  'how',\n",
       "  'they',\n",
       "  'used',\n",
       "  'to',\n",
       "  'make',\n",
       "  'movies',\n",
       "  ',',\n",
       "  'but',\n",
       "  'also',\n",
       "  'how',\n",
       "  'they',\n",
       "  'sometimes',\n",
       "  'still',\n",
       "  'can',\n",
       "  'be',\n",
       "  'made',\n",
       "  '.'],\n",
       " 'pos')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature sets for the documents. \n",
    "featuresets = [(document_features(d,word_features), c) for (d,c) in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.759\n"
     ]
    }
   ],
   "source": [
    "# We create the training and test sets, train a Naïve Bayes classifier, and look at the accuracy, \n",
    "# and this time we’ll do a 90/10 split of our approximately 10,000 documents.\n",
    "\n",
    "train_set, test_set = featuresets[1000:], featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "            V_engrossing = True              pos : neg    =     20.6 : 1.0\n",
      "             V_wonderful = True              pos : neg    =     16.6 : 1.0\n",
      "               V_generic = True              neg : pos    =     16.1 : 1.0\n",
      "             V_inventive = True              pos : neg    =     15.9 : 1.0\n",
      "              V_mediocre = True              neg : pos    =     15.4 : 1.0\n",
      "            V_refreshing = True              pos : neg    =     13.9 : 1.0\n",
      "                V_boring = True              neg : pos    =     13.1 : 1.0\n",
      "               V_routine = True              neg : pos    =     12.8 : 1.0\n",
      "                    V_90 = True              neg : pos    =     12.8 : 1.0\n",
      "                  V_flat = True              neg : pos    =     12.4 : 1.0\n",
      "                  V_warm = True              pos : neg    =     12.4 : 1.0\n",
      "                  V_dull = True              neg : pos    =     12.1 : 1.0\n",
      "                 V_stale = True              neg : pos    =     11.5 : 1.0\n",
      "          V_refreshingly = True              pos : neg    =     11.2 : 1.0\n",
      "              V_mindless = True              neg : pos    =     10.8 : 1.0\n",
      "                V_stupid = True              neg : pos    =     10.8 : 1.0\n",
      "                V_beauty = True              pos : neg    =     10.8 : 1.0\n",
      "              V_captures = True              pos : neg    =     10.8 : 1.0\n",
      "              V_provides = True              pos : neg    =     10.8 : 1.0\n",
      "              V_touching = True              pos : neg    =     10.6 : 1.0\n",
      "             V_realistic = True              pos : neg    =     10.5 : 1.0\n",
      "                  V_ages = True              pos : neg    =     10.5 : 1.0\n",
      "               V_culture = True              pos : neg    =      9.6 : 1.0\n",
      "              V_tiresome = True              neg : pos    =      9.5 : 1.0\n",
      "              V_powerful = True              pos : neg    =      9.1 : 1.0\n",
      "                   V_wry = True              pos : neg    =      9.1 : 1.0\n",
      "              V_annoying = True              neg : pos    =      8.9 : 1.0\n",
      "                 V_waste = True              neg : pos    =      8.9 : 1.0\n",
      "            V_meandering = True              neg : pos    =      8.9 : 1.0\n",
      "                  V_loud = True              neg : pos    =      8.9 : 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The function show_most_informative_features shows the top ranked features according to the ratio of one\n",
    "# label to the other one.  For example, if there are 20 times as many positive documents containing this word as negative ones,\n",
    "# then the ratio will be reported as     20.00: 1.00   pos:neg.\n",
    "\n",
    "classifier.show_most_informative_features(30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2:  Adding Features from a Sentiment Lexicon\n",
    "\n",
    "### Continuing our session with the movie review sentences\n",
    "\n",
    "### Sentiment Lexicon:  Subjectivity Count features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first read in the subjectivity words from the subjectivity lexicon file created by Janyce Wiebe and her group at the University of Pittsburgh in the MPQA project.  Although these words are often used as features themselves or in conjunction with other information, we will create two features that involve counting the positive and negative subjectivity words present in each document.\n",
    "\n",
    "Copy and paste the definition of the readSubjectivity function from the Subjectivity.txt file.  We’ll look at the function to see how it reads the file into a dictionary.\n",
    "\n",
    "Create a path variable to where you stored the subjectivity lexicon file.  Here is an example from my mac, making sure the path name goes on one line:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module Subjectivity reads the subjectivity lexicon file from Wiebe et al\n",
    "#    at http://www.cs.pitt.edu/mpqa/ (part of the Multiple Perspective QA project)\n",
    "#\n",
    "# This file has the format that each line is formatted as in this example for the word \"abandoned\"\n",
    "#     type=weaksubj len=1 word1=abandoned pos1=adj stemmed1=n priorpolarity=negative\n",
    "# In our data, the pos tag is ignored, so this program just takes the last one read\n",
    "#     (typically the noun over the adjective)\n",
    "#\n",
    "# The data structure that is created is a dictionary where\n",
    "#    each word is mapped to a list of 4 things:  \n",
    "#        strength, which will be either 'strongsubj' or 'weaksubj'\n",
    "#        posTag, either 'adj', 'verb', 'noun', 'adverb', 'anypos'\n",
    "#        isStemmed, either true or false\n",
    "#        polarity, either 'positive', 'negative', or 'neutral'\n",
    "\n",
    "import nltk\n",
    "\n",
    "# pass the absolute path of the lexicon file to this program\n",
    "# example call:\n",
    "SLpath = \"C:\\\\Users\\\\rkrishnan\\\\Documents\\\\01 Personal\\\\MS\\\\IST664\\\\Week8\\\\subjclueslen1-hltemnlp05\\\\subjclueslen1-HLTEMNLP05.tff\"\n",
    "\n",
    "\n",
    "# this function returns a dictionary where you can look up words and get back \n",
    "#     the four items of subjectivity information described above\n",
    "def readSubjectivity(path):\n",
    "    flexicon = open(path, 'r')\n",
    "    # initialize an empty dictionary\n",
    "    sldict = { }\n",
    "    for line in flexicon:\n",
    "        fields = line.split()   # default is to split on whitespace\n",
    "        # split each field on the '=' and keep the second part as the value\n",
    "        strength = fields[0].split(\"=\")[1]\n",
    "        word = fields[2].split(\"=\")[1]\n",
    "        posTag = fields[3].split(\"=\")[1]\n",
    "        stemmed = fields[4].split(\"=\")[1]\n",
    "        polarity = fields[5].split(\"=\")[1]\n",
    "        if (stemmed == 'y'):\n",
    "            isStemmed = True\n",
    "        else:\n",
    "            isStemmed = False\n",
    "        # put a dictionary entry with the word as the keyword\n",
    "        #     and a list of the other values\n",
    "        sldict[word] = [strength, posTag, isStemmed, polarity]\n",
    "    return sldict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run the function that reads the file.  It creates a Subjectivity Lexicon that is represented here as a dictionary, \n",
    "# where each word is mapped to a list containing the strength, POStag, whether it is stemmed and the polarity.  \n",
    "# (See more details in the Subjectivity.py file.)\n",
    "SL = readSubjectivity(SLpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the variable SL (for Subjectivity Lexicon) is a dictionary where you can look up words and find the strength, POS tag,\n",
    "# whether it is stemmed and polarity.  We can try out some words.\n",
    "SL['absolute']\n",
    "SL['shabby']\n",
    "# Or we can use the Python multiple assignment to get the 4 items:\n",
    "strength, posTag, isStemmed, polarity = SL['absolute']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a feature extraction function that has all the word features as before, but also has two features ‘positivecount’ and ‘negativecount’.  These features contains counts of all the positive and negative subjectivity words, where each weakly subjective word is counted once and each strongly subjective word is counted twice.  Note that this is only one of the ways in which people count up the presence of positive, negative and neutral words in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SL_features(document, SL, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "        # count variables for the 4 classes of subjectivity\n",
    "        weakPos = 0\n",
    "        strongPos = 0\n",
    "        weakNeg = 0\n",
    "        strongNeg = 0\n",
    "        for word in document_words:\n",
    "            if word in SL:\n",
    "                strength, posTag, isStemmed, polarity = SL[word]\n",
    "                if strength == 'weaksubj' and polarity == 'positive':\n",
    "                    weakPos += 1\n",
    "                if strength == 'strongsubj' and polarity == 'positive':\n",
    "                    strongPos += 1\n",
    "                if strength == 'weaksubj' and polarity == 'negative':\n",
    "                    weakNeg += 1\n",
    "                if strength == 'strongsubj' and polarity == 'negative':\n",
    "                    strongNeg += 1\n",
    "                features['positivecount'] = weakPos + (2 * strongPos)\n",
    "                features['negativecount'] = weakNeg + (2 * strongNeg)      \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create feature sets as before, but using this feature extraction function.\n",
    "\n",
    "SL_featuresets = [(SL_features(d, SL,word_features), c) for (d,c) in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "pos\n",
      "0.773\n"
     ]
    }
   ],
   "source": [
    "# features in document 0\n",
    "print(SL_featuresets[0][0]['positivecount'])\n",
    "\n",
    "print(SL_featuresets[0][0]['negativecount'])\n",
    "\n",
    "print(SL_featuresets[0][1])\n",
    "\n",
    "train_set, test_set = SL_featuresets[1000:], SL_featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my random training, test split, these particular sentiment features did improve the classification on this dataset.  But also note that there are several different ways to represent features for a sentiment lexicon, e.g. instead of counting the sentiment words, we could get one overall score by subtracting the number of negative words from positive words, or other ways to score the sentiment words.  Also note that there are many different sentiment lexicons to try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3:  Adding Negation Features \n",
    "\n",
    "### Continuing our session with the movie review sentences\n",
    "\n",
    "### Negation features\n",
    "\n",
    "Negation of opinions is an important part of opinion classification.  Here we try a simple strategy.  We look for negation words \"not\", \"never\" and \"no\" and negation that appears in contractions of the form \"doesn’t\".\n",
    "\n",
    "One strategy with negation words is to negate the word following the negation word, while other strategies negate all words up to the next punctuation or use syntax to find the scope of the negation.\n",
    "\n",
    "We follow the first strategy here, and we go through the document words in order adding the word features, but if the word follows a negation words, change the feature to negated word.\n",
    "\n",
    "Here is one list of negation words, including some adverbs called “approximate negators”:\n",
    "no, not, never, none, rather, hardly, scarcely, rarely, seldom, neither, nor,\n",
    "couldn't, wasn't, didn't, wouldn't, shouldn't, weren't, don't, doesn't, haven't, hasn't, won't, hadn't\n",
    "\n",
    "The form of some of the words is a verb followed by n’t.  Now in the Movie Review Corpus itself, the tokenization has these words all split into 3 words, e.g. “couldn”, “’”, and “t”.  (and I have a NOT_features definition for this case).  But in this sentence_polarity corpus, the tokenization keeps these forms of negation as one word ending in “n’t”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['there', 'is', 'a', 'difference', 'between', 'movies', 'with', 'the', 'courage', 'to', 'go', 'over', 'the', 'top', 'and', 'movies', 'that', \"don't\", 'care', 'about', 'being', 'stupid']\n",
      "['a', 'farce', 'of', 'a', 'parody', 'of', 'a', 'comedy', 'of', 'a', 'premise', ',', 'it', \"isn't\", 'a', 'comparison', 'to', 'reality', 'so', 'much', 'as', 'it', 'is', 'a', 'commentary', 'about', 'our', 'knowledge', 'of', 'films', '.']\n",
      "['i', \"didn't\", 'laugh', '.', 'i', \"didn't\", 'smile', '.', 'i', 'survived', '.']\n",
      "['i', \"didn't\", 'laugh', '.', 'i', \"didn't\", 'smile', '.', 'i', 'survived', '.']\n",
      "['most', 'of', 'the', 'problems', 'with', 'the', 'film', \"don't\", 'derive', 'from', 'the', 'screenplay', ',', 'but', 'rather', 'the', 'mediocre', 'performances', 'by', 'most', 'of', 'the', 'actors', 'involved']\n",
      "['the', 'lack', 'of', 'naturalness', 'makes', 'everything', 'seem', 'self-consciously', 'poetic', 'and', 'forced', '.', '.', '.', \"it's\", 'a', 'pity', 'that', \"[nelson's]\", 'achievement', \"doesn't\", 'match', 'his', 'ambition', '.']\n"
     ]
    }
   ],
   "source": [
    "for sent in list(sentences)[:50]:\n",
    "    for word in sent:\n",
    "        if (word.endswith(\"n't\")):\n",
    "            print(sent)\n",
    "\n",
    "negationwords = ['no', 'not', 'never', 'none', 'nowhere', 'nothing', 'noone', 'rather', 'hardly', 'scarcely', \\\n",
    "                 'rarely', 'seldom', 'neither', 'nor']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the feature set with all 2000 word features and 2000 Not word features set to false.  If a negation occurs, add the following word as a Not word feature (if it’s in the top 2000 feature words), and otherwise add it as a regular feature word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One strategy with negation words is to negate the word following the negation word\n",
    "#   other strategies negate all words up to the next punctuation\n",
    "# Strategy is to go through the document words in order adding the word features,\n",
    "#   but if the word follows a negation words, change the feature to negated word\n",
    "# Start the feature set with all 2000 word features and 2000 Not word features set to false\n",
    "def NOT_features(document, word_features, negationwords):\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = False\n",
    "        features['V_NOT{}'.format(word)] = False\n",
    "    # go through document words in order\n",
    "    for i in range(0, len(document)):\n",
    "        word = document[i]\n",
    "        if ((i + 1) < len(document)) and ((word in negationwords) or (word.endswith(\"n't\"))):\n",
    "            i += 1\n",
    "            features['V_NOT{}'.format(document[i])] = (document[i] in word_features)\n",
    "        else:\n",
    "            features['V_{}'.format(word)] = (word in word_features)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "0.796\n",
      "Most Informative Features\n",
      "            V_engrossing = True              pos : neg    =     20.6 : 1.0\n",
      "             V_wonderful = True              pos : neg    =     16.6 : 1.0\n",
      "               V_generic = True              neg : pos    =     16.1 : 1.0\n",
      "             V_inventive = True              pos : neg    =     15.9 : 1.0\n",
      "              V_mediocre = True              neg : pos    =     15.4 : 1.0\n",
      "            V_refreshing = True              pos : neg    =     13.9 : 1.0\n",
      "                V_boring = True              neg : pos    =     13.1 : 1.0\n",
      "                    V_90 = True              neg : pos    =     12.8 : 1.0\n",
      "               V_routine = True              neg : pos    =     12.8 : 1.0\n",
      "                  V_flat = True              neg : pos    =     12.4 : 1.0\n",
      "                  V_warm = True              pos : neg    =     12.4 : 1.0\n",
      "                  V_dull = True              neg : pos    =     12.1 : 1.0\n",
      "             V_NOTenough = True              neg : pos    =     11.5 : 1.0\n",
      "                 V_stale = True              neg : pos    =     11.5 : 1.0\n",
      "          V_refreshingly = True              pos : neg    =     11.2 : 1.0\n",
      "              V_mindless = True              neg : pos    =     10.8 : 1.0\n",
      "                V_stupid = True              neg : pos    =     10.8 : 1.0\n",
      "              V_provides = True              pos : neg    =     10.8 : 1.0\n",
      "                V_beauty = True              pos : neg    =     10.8 : 1.0\n",
      "              V_captures = True              pos : neg    =     10.8 : 1.0\n",
      "              V_touching = True              pos : neg    =     10.6 : 1.0\n",
      "             V_realistic = True              pos : neg    =     10.5 : 1.0\n",
      "                  V_ages = True              pos : neg    =     10.5 : 1.0\n",
      "               V_culture = True              pos : neg    =      9.6 : 1.0\n",
      "              V_tiresome = True              neg : pos    =      9.5 : 1.0\n",
      "              V_powerful = True              pos : neg    =      9.1 : 1.0\n",
      "                   V_wry = True              pos : neg    =      9.1 : 1.0\n",
      "                  V_loud = True              neg : pos    =      8.9 : 1.0\n",
      "            V_meandering = True              neg : pos    =      8.9 : 1.0\n",
      "              V_annoying = True              neg : pos    =      8.9 : 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create feature sets as before, using the NOT_features extraction funtion, train the classifier and test the accuracy.\n",
    "NOT_featuresets = [(NOT_features(d, word_features, negationwords), c) for (d, c) in documents]\n",
    "print(NOT_featuresets[0][0]['V_NOTcare'])\n",
    "print(NOT_featuresets[0][0]['V_always'])\n",
    "\n",
    "train_set, test_set = NOT_featuresets[1000:], NOT_featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "\n",
    "classifier.show_most_informative_features(30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my random split, using the negation features did improve the classification.\n",
    "\n",
    "\n",
    "Other features\n",
    "\n",
    "There are other types of possible features.  For example, sometimes people use bigrams in addition to just words/unigrams or use the counts of POS tags, which we will look at next week.  Also, there are many other forms of negation features.\n",
    "\n",
    "For some problems, the word features can be pruned with a stop word list, but care should be taken that the list doesn’t remove any negation or useful function words.  A very small stop word list is probably better than a large one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "157\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', \"aren't\", \"couldn't\", \"didn't\", \"doesn't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\", \"needn't\", \"shan't\", \"shouldn't\", \"wasn't\", \"weren't\", \"won't\", \"wouldn't\"]\n",
      "['.', ',', 'film', 'movie', 'not', 'one', 'like', '\"', '--', 'story', 'no', 'much', 'even', 'good', 'comedy', 'time', 'characters', 'little', 'way', 'funny', 'make', 'enough', 'never', 'makes', 'may', 'us', 'work', 'best', 'bad', 'director']\n"
     ]
    }
   ],
   "source": [
    "### Bonus python text for the Question, define a stop word list ###\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print(len(stopwords))\n",
    "print(stopwords)\n",
    "\n",
    "# remove some negation words \n",
    "negationwords.extend(['ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn'])\n",
    "\n",
    "newstopwords = [word for word in stopwords if word not in negationwords]\n",
    "print(len(newstopwords))\n",
    "print(newstopwords)\n",
    "\n",
    "# remove stop words from the all words list\n",
    "new_all_words_list = [word for (sent,cat) in documents for word in sent if word not in newstopwords]\n",
    "\n",
    "# continue to define a new all words dictionary, get the 2000 most common as new_word_features\n",
    "new_all_words = nltk.FreqDist(new_all_words_list)\n",
    "new_word_items = new_all_words.most_common(2000)\n",
    "\n",
    "new_word_features = [word for (word,count) in new_word_items]\n",
    "print(new_word_features[:30])\n",
    "\n",
    "# now re-run one of the feature set definitions with the new_word_features instead of word_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "2002\n",
      "4001\n"
     ]
    }
   ],
   "source": [
    "print(len(featuresets[0][0]))\n",
    "print(len(SL_featuresets[0][0]))\n",
    "print(len(NOT_featuresets[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10662\n",
      "0.755\n",
      "10662\n",
      "0.771\n",
      "10662\n",
      "0.796\n",
      "Most Informative Features\n",
      "            V_engrossing = True              pos : neg    =     20.6 : 1.0\n",
      "             V_wonderful = True              pos : neg    =     16.6 : 1.0\n",
      "               V_generic = True              neg : pos    =     16.1 : 1.0\n",
      "             V_inventive = True              pos : neg    =     15.9 : 1.0\n",
      "              V_mediocre = True              neg : pos    =     15.4 : 1.0\n",
      "            V_refreshing = True              pos : neg    =     13.9 : 1.0\n",
      "                V_boring = True              neg : pos    =     13.1 : 1.0\n",
      "                    V_90 = True              neg : pos    =     12.8 : 1.0\n",
      "               V_routine = True              neg : pos    =     12.8 : 1.0\n",
      "                  V_flat = True              neg : pos    =     12.4 : 1.0\n",
      "                  V_warm = True              pos : neg    =     12.4 : 1.0\n",
      "                  V_dull = True              neg : pos    =     12.1 : 1.0\n",
      "             V_NOTenough = True              neg : pos    =     11.5 : 1.0\n",
      "                 V_stale = True              neg : pos    =     11.5 : 1.0\n",
      "          V_refreshingly = True              pos : neg    =     11.2 : 1.0\n",
      "              V_mindless = True              neg : pos    =     10.8 : 1.0\n",
      "                V_stupid = True              neg : pos    =     10.8 : 1.0\n",
      "              V_provides = True              pos : neg    =     10.8 : 1.0\n",
      "                V_beauty = True              pos : neg    =     10.8 : 1.0\n",
      "              V_captures = True              pos : neg    =     10.8 : 1.0\n",
      "              V_touching = True              pos : neg    =     10.6 : 1.0\n",
      "             V_realistic = True              pos : neg    =     10.5 : 1.0\n",
      "                  V_ages = True              pos : neg    =     10.5 : 1.0\n",
      "               V_culture = True              pos : neg    =      9.6 : 1.0\n",
      "              V_tiresome = True              neg : pos    =      9.5 : 1.0\n",
      "              V_powerful = True              pos : neg    =      9.1 : 1.0\n",
      "                   V_wry = True              pos : neg    =      9.1 : 1.0\n",
      "                  V_loud = True              neg : pos    =      8.9 : 1.0\n",
      "            V_meandering = True              neg : pos    =      8.9 : 1.0\n",
      "              V_annoying = True              neg : pos    =      8.9 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Define the feature sets for the documents with the new word features\n",
    "featuresets = [(document_features(d,new_word_features), c) for (d,c) in documents]\n",
    "print(len(featuresets))\n",
    "train_set, test_set = featuresets[1000:], featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "\n",
    "# Now we create feature sets as before, but using this feature extraction function with the new word features\n",
    "SL_featuresets = [(SL_features(d, SL,new_word_features), c) for (d,c) in documents]\n",
    "print(len(SL_featuresets))\n",
    "train_set, test_set = SL_featuresets[1000:], SL_featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "\n",
    "# Create feature sets as before, using the NOT_features extraction funtion, train the classifier and test the accuracy  with the new word features\n",
    "NOT_featuresets = [(NOT_features(d, new_word_features, negationwords), c) for (d, c) in documents]\n",
    "print(len(NOT_featuresets))\n",
    "train_set, test_set = NOT_featuresets[1000:], NOT_featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "\n",
    "classifier.show_most_informative_features(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
