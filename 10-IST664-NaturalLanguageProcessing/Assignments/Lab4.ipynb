{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### NLP Lab Session Week 4\n",
    "### POS Taggers in the NLTK\n",
    "### Part 1:  Session Setup and Tagged Corpora\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Getting Started\n",
    "\n",
    "In this lab session, we will work together through a series of small examples using the Python interpreter in Jupyter notebook.  As before, you may use the text file with the Python examples.  \n",
    "\n",
    "Download LabWeek4.POStags.txt\n",
    "\n",
    "Save it in a folder where you keep materials for this class.   Open your command prompt or terminal window and use the cd command to change directory to your class materials folder.  Type at the prompt:\n",
    "\n",
    "$ jupyter notebook\n",
    "\n",
    "As usual, start your nlp session by:\n",
    "\n",
    "import nltk\n",
    "\n",
    "Reading Tagged Corpora\n",
    "\n",
    "The NLTK corpus readers have additional methods (aka functions) that can give the additional tag information from reading a tagged corpus.  Both the Brown corpus and the Penn Treebank corpus have text in which each token has been tagged with a POS tag.  (These were manually assigned by annotators.)\n",
    "\n",
    "The tagged_sents function gives a list of sentences, each sentence is a list of (word, tag) tuples. We’ll first look at the Brown corpus, which is described in Chapter 2 of the NLTK book.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('The', 'AT'),\n",
       "  ('Fulton', 'NP-TL'),\n",
       "  ('County', 'NN-TL'),\n",
       "  ('Grand', 'JJ-TL'),\n",
       "  ('Jury', 'NN-TL'),\n",
       "  ('said', 'VBD'),\n",
       "  ('Friday', 'NR'),\n",
       "  ('an', 'AT'),\n",
       "  ('investigation', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  (\"Atlanta's\", 'NP$'),\n",
       "  ('recent', 'JJ'),\n",
       "  ('primary', 'NN'),\n",
       "  ('election', 'NN'),\n",
       "  ('produced', 'VBD'),\n",
       "  ('``', '``'),\n",
       "  ('no', 'AT'),\n",
       "  ('evidence', 'NN'),\n",
       "  (\"''\", \"''\"),\n",
       "  ('that', 'CS'),\n",
       "  ('any', 'DTI'),\n",
       "  ('irregularities', 'NNS'),\n",
       "  ('took', 'VBD'),\n",
       "  ('place', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('The', 'AT'),\n",
       "  ('jury', 'NN'),\n",
       "  ('further', 'RBR'),\n",
       "  ('said', 'VBD'),\n",
       "  ('in', 'IN'),\n",
       "  ('term-end', 'NN'),\n",
       "  ('presentments', 'NNS'),\n",
       "  ('that', 'CS'),\n",
       "  ('the', 'AT'),\n",
       "  ('City', 'NN-TL'),\n",
       "  ('Executive', 'JJ-TL'),\n",
       "  ('Committee', 'NN-TL'),\n",
       "  (',', ','),\n",
       "  ('which', 'WDT'),\n",
       "  ('had', 'HVD'),\n",
       "  ('over-all', 'JJ'),\n",
       "  ('charge', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('election', 'NN'),\n",
       "  (',', ','),\n",
       "  ('``', '``'),\n",
       "  ('deserves', 'VBZ'),\n",
       "  ('the', 'AT'),\n",
       "  ('praise', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('thanks', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('City', 'NN-TL'),\n",
       "  ('of', 'IN-TL'),\n",
       "  ('Atlanta', 'NP-TL'),\n",
       "  (\"''\", \"''\"),\n",
       "  ('for', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('manner', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('which', 'WDT'),\n",
       "  ('the', 'AT'),\n",
       "  ('election', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('conducted', 'VBN'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "brown.tagged_sents()[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tagged_words function just gives a list of all the (word, tag) tuples, ignoring the sentence structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('Fulton', 'NP-TL'),\n",
       " ('County', 'NN-TL'),\n",
       " ('Grand', 'JJ-TL'),\n",
       " ('Jury', 'NN-TL'),\n",
       " ('said', 'VBD'),\n",
       " ('Friday', 'NR'),\n",
       " ('an', 'AT'),\n",
       " ('investigation', 'NN'),\n",
       " ('of', 'IN'),\n",
       " (\"Atlanta's\", 'NP$'),\n",
       " ('recent', 'JJ'),\n",
       " ('primary', 'NN'),\n",
       " ('election', 'NN'),\n",
       " ('produced', 'VBD'),\n",
       " ('``', '``'),\n",
       " ('no', 'AT'),\n",
       " ('evidence', 'NN'),\n",
       " (\"''\", \"''\"),\n",
       " ('that', 'CS'),\n",
       " ('any', 'DTI'),\n",
       " ('irregularities', 'NNS'),\n",
       " ('took', 'VBD'),\n",
       " ('place', 'NN'),\n",
       " ('.', '.'),\n",
       " ('The', 'AT'),\n",
       " ('jury', 'NN'),\n",
       " ('further', 'RBR'),\n",
       " ('said', 'VBD'),\n",
       " ('in', 'IN'),\n",
       " ('term-end', 'NN'),\n",
       " ('presentments', 'NNS'),\n",
       " ('that', 'CS'),\n",
       " ('the', 'AT'),\n",
       " ('City', 'NN-TL'),\n",
       " ('Executive', 'JJ-TL'),\n",
       " ('Committee', 'NN-TL'),\n",
       " (',', ','),\n",
       " ('which', 'WDT'),\n",
       " ('had', 'HVD'),\n",
       " ('over-all', 'JJ'),\n",
       " ('charge', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'AT'),\n",
       " ('election', 'NN'),\n",
       " (',', ','),\n",
       " ('``', '``'),\n",
       " ('deserves', 'VBZ'),\n",
       " ('the', 'AT'),\n",
       " ('praise', 'NN')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.tagged_words()[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We said that each of these is what Python calls a tuple, which is a pair (or triple, etc.) in which you can’t change the elements.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordtag = brown.tagged_words()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The', 'AT')\n",
      "The\n",
      "AT\n"
     ]
    }
   ],
   "source": [
    "print(wordtag)\n",
    "\n",
    "print(wordtag[0])\n",
    "\n",
    "print(wordtag[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Brown corpus is organized into different types of text, which can be selected by the categories argument, and it also allows you to map the tags to a simplified tag set, described in table 5.1 in the NLTK book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('It', 'PRON'),\n",
       " ('was', 'VERB'),\n",
       " ('among', 'ADP'),\n",
       " ('these', 'DET'),\n",
       " ('that', 'ADP'),\n",
       " ('Hinkle', 'NOUN'),\n",
       " ('identified', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('photograph', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('Barco', 'NOUN'),\n",
       " ('!', '.'),\n",
       " ('!', '.'),\n",
       " ('For', 'ADP'),\n",
       " ('it', 'PRON'),\n",
       " ('seems', 'VERB'),\n",
       " ('that', 'ADP'),\n",
       " ('Barco', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('fancying', 'VERB'),\n",
       " ('himself', 'PRON'),\n",
       " ('a', 'DET'),\n",
       " (\"ladies'\", 'NOUN'),\n",
       " ('man', 'NOUN'),\n",
       " ('(', '.'),\n",
       " ('and', 'CONJ'),\n",
       " ('why', 'ADV'),\n",
       " ('not', 'ADV'),\n",
       " (',', '.'),\n",
       " ('after', 'ADP'),\n",
       " ('seven', 'NUM'),\n",
       " ('marriages', 'NOUN'),\n",
       " ('?', '.'),\n",
       " ('?', '.'),\n",
       " (')', '.'),\n",
       " (',', '.'),\n",
       " ('had', 'VERB'),\n",
       " ('listed', 'VERB'),\n",
       " ('himself', 'PRON'),\n",
       " ('for', 'ADP'),\n",
       " ('Mormon', 'NOUN'),\n",
       " ('Beard', 'NOUN'),\n",
       " ('roles', 'NOUN'),\n",
       " ('at', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('instigation', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('his', 'DET'),\n",
       " ('fourth', 'ADJ'),\n",
       " ('murder', 'NOUN')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.categories()\n",
    "brown_humor_tagged = brown.tagged_words(categories='humor', tagset='universal')\n",
    "brown_humor_tagged[:50]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other tagged corpora also come with the tagged_words method.  Note that the chat corpus is tagged with Penn Treebank POS tags.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('now', 'RB'),\n",
       " ('im', 'PRP'),\n",
       " ('left', 'VBD'),\n",
       " ('with', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('gay', 'JJ'),\n",
       " ('name', 'NN'),\n",
       " (':P', 'UH'),\n",
       " ('PART', 'VB'),\n",
       " ('hey', 'UH'),\n",
       " ('everyone', 'NN'),\n",
       " ('ah', 'UH'),\n",
       " ('well', 'UH'),\n",
       " ('NICK', 'NN'),\n",
       " (':', ':'),\n",
       " ('U7', 'NNP'),\n",
       " ('U7', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('gay', 'JJ'),\n",
       " ('name', 'NN'),\n",
       " ('.', '.'),\n",
       " ('.', 'SYM'),\n",
       " ('ACTION', 'NN'),\n",
       " ('gives', 'VBZ'),\n",
       " ('U121', 'NNP'),\n",
       " ('a', 'DT'),\n",
       " ('golf', 'NN'),\n",
       " ('clap', 'NN'),\n",
       " ('.', '.'),\n",
       " (':)', 'UH'),\n",
       " ('JOIN', 'VB'),\n",
       " ('hi', 'UH'),\n",
       " ('U59', 'NNP'),\n",
       " ('26', 'CD'),\n",
       " ('/', 'CC'),\n",
       " ('m', 'NN'),\n",
       " ('/', 'CC'),\n",
       " ('ky', 'NNP'),\n",
       " ('women', 'NNS'),\n",
       " ('that', 'WDT'),\n",
       " ('are', 'VBP'),\n",
       " ('nice', 'JJ'),\n",
       " ('please', 'VB'),\n",
       " ('pm', 'VB'),\n",
       " ('me', 'PRP'),\n",
       " ('JOIN', 'VB'),\n",
       " ('PART', 'VB'),\n",
       " ('there', 'RB'),\n",
       " ('ya', 'PRP')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.nps_chat.tagged_words()[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penn Treebank\n",
    "\n",
    "In this class, we will mostly use the Penn Treebank tag set, as it is the most widely used.  The Treebank has the tagged_words and tagged_sents methods, as well as the words method that we used before to get the tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "( (S \n",
      "    (NP-SBJ \n",
      "      (NP (NNP Pierre) (NNP Vinken) )\n",
      "      (, ,) \n",
      "      (ADJP \n",
      "        (NP (CD 61) (NNS years) )\n",
      "        (JJ old) )\n",
      "      (, ,) ) \n",
      "\n",
      "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.', 'Mr.', 'Vinken']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "# the .raw() and .words() functions still get the text as strings and as tokens\n",
    "treebank_text = treebank.raw()\n",
    "print(treebank_text[:150], '\\n')\n",
    "treebank_tokens = treebank.words()\n",
    "print(treebank_tokens[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Pierre', 'NNP'),\n",
       "  ('Vinken', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('61', 'CD'),\n",
       "  ('years', 'NNS'),\n",
       "  ('old', 'JJ'),\n",
       "  (',', ','),\n",
       "  ('will', 'MD'),\n",
       "  ('join', 'VB'),\n",
       "  ('the', 'DT'),\n",
       "  ('board', 'NN'),\n",
       "  ('as', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('nonexecutive', 'JJ'),\n",
       "  ('director', 'NN'),\n",
       "  ('Nov.', 'NNP'),\n",
       "  ('29', 'CD'),\n",
       "  ('.', '.')],\n",
       " [('Mr.', 'NNP'),\n",
       "  ('Vinken', 'NNP'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('chairman', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('Elsevier', 'NNP'),\n",
       "  ('N.V.', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('the', 'DT'),\n",
       "  ('Dutch', 'NNP'),\n",
       "  ('publishing', 'VBG'),\n",
       "  ('group', 'NN'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# but we also have functions to get words with tags and sentences with tagged words\n",
    "treebank_tagged_words = treebank.tagged_words()[:50]\n",
    "len(treebank.tagged_words())\n",
    "treebank_tagged_words[:50]\n",
    "\n",
    "treebank_tagged = treebank.tagged_sents()[:2]\n",
    "len(treebank.tagged_sents())\n",
    "treebank_tagged[:2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK has almost 4,000 sentences of tagged data from Penn Treebank, while the actual Treebank has much more.  This will limit the accuracy of the POS taggers (and later parsers) that we can define in lab, but also make the running times short enough for labs.\n",
    "Let’s look at the frequencies of the tags in this portion of Penn Treebank.  To do that, we use the NLTK Frequency Distribution for all the tags from the (word, tag) pairs in the Treebank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['NNP', ',', 'CD', 'NNS', 'JJ', 'MD', 'VB', 'DT', 'NN', 'IN', '.', 'VBZ', 'VBG', 'CC', 'VBD', 'VBN', '-NONE-']) \n",
      "\n",
      "NNP 14\n",
      ", 5\n",
      "NN 5\n",
      "JJ 4\n",
      "DT 4\n",
      "CD 3\n",
      "IN 3\n",
      "NNS 2\n",
      ". 2\n",
      "MD 1\n",
      "VB 1\n",
      "VBZ 1\n",
      "VBG 1\n",
      "CC 1\n",
      "VBD 1\n",
      "VBN 1\n",
      "-NONE- 1\n"
     ]
    }
   ],
   "source": [
    "tag_fd = nltk.FreqDist(tag for (word, tag) in treebank_tagged_words)\n",
    "print(tag_fd.keys(), '\\n')\n",
    "for tag,freq in tag_fd.most_common():\n",
    "    print (tag, freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that NN, the tag of single nouns, is the most frequent tag;  it has 13,166 occurrences of the 100,676 words, or about 13%.  The tags IN, for prepositions except to, NNP, for single proper nouns, and DT, for determiners, are close behind at 10%, 9% and 8%, respectively.  The next tag in the list is –NONE-, which is the tag of those empty elements, which come from the grammar syntactic constructs.\n",
    "\n",
    "This is a very detailed look at the POS tags.  We could also approximate classes of tags by using the first letter of the POS tag as the key in a frequency distribution.  For example, this will group all the nouns, which all start with “N”, all the verbs, “V”, adjectives “J”, and adverbs “R”.  The prepositions will be split between “I” for the ones that are tagged “IN” and “T” for the ones that are tagged “TO”.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['N', ',', 'C', 'J', 'M', 'V', 'D', 'I', '.', '-']) \n",
      "\n",
      "N 21\n",
      ", 5\n",
      "V 5\n",
      "C 4\n",
      "J 4\n",
      "D 4\n",
      "I 3\n",
      ". 2\n",
      "M 1\n",
      "- 1\n"
     ]
    }
   ],
   "source": [
    "# use the first letter of the POS tag to get classes of tags\n",
    "tag_classes_fd = nltk.FreqDist(tag[0] for (word, tag) in treebank_tagged_words)\n",
    "print(tag_classes_fd.keys(), '\\n')\n",
    "for tag,freq in tag_classes_fd.most_common():\n",
    "    print (tag, freq)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Part 2:  Tagger Training Setup\n",
    "\n",
    "### The POS Tagging Task and Training a default tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of classifying words into their parts of speech and labeling them accordingly is known as part-of-speech tagging, POS-tagging, or simply tagging. Parts of speech are also known as word classes or lexical categories. The collection of tags used for a particular task is known as a tagset. \n",
    "\n",
    "We will use the tagged sentences and words from the Penn Treebank that we defined in the previous section.\n",
    "\n",
    "We separate our tagged data into a training set, where we’ll learn the probabilities of the words and their tags, and a test set to evaluate how out taggers perform.  This allows us to test the tagger’s accuracy on similar, but not the same, data that it was trained on.  The training set is the first 90% of the sentences and the test set is the remaining 10%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Pierre', 'NNP'),\n",
       "  ('Vinken', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('61', 'CD'),\n",
       "  ('years', 'NNS'),\n",
       "  ('old', 'JJ'),\n",
       "  (',', ','),\n",
       "  ('will', 'MD'),\n",
       "  ('join', 'VB'),\n",
       "  ('the', 'DT'),\n",
       "  ('board', 'NN'),\n",
       "  ('as', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('nonexecutive', 'JJ'),\n",
       "  ('director', 'NN'),\n",
       "  ('Nov.', 'NNP'),\n",
       "  ('29', 'CD'),\n",
       "  ('.', '.')],\n",
       " [('Mr.', 'NNP'),\n",
       "  ('Vinken', 'NNP'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('chairman', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('Elsevier', 'NNP'),\n",
       "  ('N.V.', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('the', 'DT'),\n",
       "  ('Dutch', 'NNP'),\n",
       "  ('publishing', 'VBG'),\n",
       "  ('group', 'NN'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treebank_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = int(len(treebank_tagged) * 0.9)\n",
    "treebank_train = treebank_tagged[:size]\n",
    "treebank_test = treebank_tagged[size:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the NLTK, a number of POS taggers are included in the tag module, including one that we can use that has been trained on all of Penn Treebank.  But for instructional purposes, we will develop a sequence of N-gram taggers whose performance improves.\n",
    "\n",
    "To introduce the N-gram taggers in NLTK, we start with a default tagger that just tags everything with the most frequent tag:  NN.   We create the tagger and run it on text.  Note that this simple tagger doesn’t actually use the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NN'),\n",
       " ('Vinken', 'NN'),\n",
       " (',', 'NN'),\n",
       " ('61', 'NN'),\n",
       " ('years', 'NN'),\n",
       " ('old', 'NN'),\n",
       " (',', 'NN'),\n",
       " ('will', 'NN'),\n",
       " ('join', 'NN'),\n",
       " ('the', 'NN'),\n",
       " ('board', 'NN'),\n",
       " ('as', 'NN'),\n",
       " ('a', 'NN'),\n",
       " ('nonexecutive', 'NN'),\n",
       " ('director', 'NN'),\n",
       " ('Nov.', 'NN'),\n",
       " ('29', 'NN'),\n",
       " ('.', 'NN'),\n",
       " ('Mr.', 'NN'),\n",
       " ('Vinken', 'NN'),\n",
       " ('is', 'NN'),\n",
       " ('chairman', 'NN'),\n",
       " ('of', 'NN'),\n",
       " ('Elsevier', 'NN'),\n",
       " ('N.V.', 'NN'),\n",
       " (',', 'NN'),\n",
       " ('the', 'NN'),\n",
       " ('Dutch', 'NN'),\n",
       " ('publishing', 'NN'),\n",
       " ('group', 'NN'),\n",
       " ('.', 'NN'),\n",
       " ('Rudolph', 'NN'),\n",
       " ('Agnew', 'NN'),\n",
       " (',', 'NN'),\n",
       " ('55', 'NN'),\n",
       " ('years', 'NN'),\n",
       " ('old', 'NN'),\n",
       " ('and', 'NN'),\n",
       " ('former', 'NN'),\n",
       " ('chairman', 'NN'),\n",
       " ('of', 'NN'),\n",
       " ('Consolidated', 'NN'),\n",
       " ('Gold', 'NN'),\n",
       " ('Fields', 'NN'),\n",
       " ('PLC', 'NN'),\n",
       " (',', 'NN'),\n",
       " ('was', 'NN'),\n",
       " ('named', 'NN'),\n",
       " ('*-1', 'NN'),\n",
       " ('a', 'NN')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creates the tagger\n",
    "t0 = nltk.DefaultTagger('NN')\n",
    "# show the effect of the tagger by tagging the first 50 words\n",
    "t0.tag(treebank_tokens[:50])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK includes a function for taggers that computes tagging accuracy by comparing the result of a tagger with the original “gold standard” tagged text.  Here we use the NLTK function “evaluate” to apply the default tagger (to the untagged text) and compare it with the gold standard tagged text in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15384615384615385"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0.evaluate(treebank_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluate function first takes the tagged text and removes the tags, so that only tokens are left.  Then it runs the tagger, in this case t0, to tag all the text.  Then it compares the tags predicted by the tagger with the “gold standard” tags already given.  It reports the accuracy, which is the percentage of words with correct tags.\n",
    "\tGiven a trained tagger, the evaluate function:\n",
    "\t\tTakes a test set consisting of text words with “correct” tags\n",
    "\t\tCreates a predicted test set by\n",
    "\t\t\tremoving tags from the test set\n",
    "\t\t\trunning the tagger to get predicted tags\n",
    "\t\tCompares the correct tag test set with the predicted tag test set\n",
    "\t\t\tand reports accuracy\n",
    "\n",
    "\n",
    "Other simple taggers described in the NLTK book are the Regular Expression Tagger and the Lookup Tagger.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3:  Training the N-Gram Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue our development of training a tagger by training a Unigram tagger.  It tags each word with the most frequent tag in that word has in the corpus.  For example, if the word “bank” occurs 30 times with the tag “NN” and 10 times with the tag “VB”, we’ll just tag it with “NN”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NNP'),\n",
       " ('Vinken', 'NNP'),\n",
       " (',', ','),\n",
       " ('61', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('old', 'JJ'),\n",
       " (',', ','),\n",
       " ('will', 'MD'),\n",
       " ('join', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('board', 'NN'),\n",
       " ('as', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('nonexecutive', 'JJ'),\n",
       " ('director', 'NN'),\n",
       " ('Nov.', 'NNP'),\n",
       " ('29', 'CD'),\n",
       " ('.', '.'),\n",
       " ('Mr.', 'NNP'),\n",
       " ('Vinken', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('chairman', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('Elsevier', 'NNP'),\n",
       " ('N.V.', 'NNP'),\n",
       " (',', ','),\n",
       " ('the', 'DT'),\n",
       " ('Dutch', 'NNP'),\n",
       " ('publishing', 'VBG'),\n",
       " ('group', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Rudolph', None),\n",
       " ('Agnew', None),\n",
       " (',', ','),\n",
       " ('55', None),\n",
       " ('years', 'NNS'),\n",
       " ('old', 'JJ'),\n",
       " ('and', None),\n",
       " ('former', None),\n",
       " ('chairman', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('Consolidated', None),\n",
       " ('Gold', None),\n",
       " ('Fields', None),\n",
       " ('PLC', None),\n",
       " (',', ','),\n",
       " ('was', None),\n",
       " ('named', None),\n",
       " ('*-1', None),\n",
       " ('a', 'DT')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = nltk.UnigramTagger(treebank_tagged)\n",
    "t1.tag(treebank_tokens[:50])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the tagger on the training set and evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3076923076923077"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = nltk.UnigramTagger(treebank_train)\n",
    "t1.evaluate(treebank_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture slides, this Unigram Tagger is what Chris Manning called their baseline tagger and they got about 90% accuracy.  Why isn’t ours quite as good?\n",
    "\n",
    "Finally, NLTK has a Bigram tagger that can be trained using 2 tag-word sequences. \n",
    "But there will be unknown frequencies in the test data for the bigram tagger, and unknown words for the unigram tagger, so we can use the backoff tagger capability of NLTK to create a combined tagger.  This tagger uses bigram frequencies to tag as much as possible.  If a word doesn’t occur in a bigram, it uses the unigram tagger to tag that word.  If the word is unknown to the unigram tagger, then we use the default tagger to tag it as ‘NN’.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46153846153846156"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = nltk.DefaultTagger('NN')\n",
    "t1 = nltk.UnigramTagger(treebank_train, backoff=t0)\n",
    "t2 = nltk.BigramTagger(treebank_train, backoff=t1)\n",
    "t2.evaluate(treebank_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This accuracy is not bad, especially on only part of Penn Treebank!  We know that HMM and other feature techniques can raise the accuracy to between 95 and 98%.  \n",
    "\n",
    "But note that this good performance is also on a test set taken from the Penn Treebank, where there may not be very many unknown words.  More modern text or text on different topics than the Wall Street Journal will have more difficulty with unknown words.  We know from the lectures that combining this with a regular expression tagger or a classifier tagger can improve performance on unknown words.\n",
    "\n",
    "Save your tagger t2 for the next section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4:  Using a Tagger to Tag Text\n",
    "\n",
    "### Applying the N-Gram tagger to text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s use the N-gram tagger that we trained in the previous section to tag some example text.  We will define some example text, tokenize it, and apply the tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Three Calgarians have found a rather unusual way of leaving snow and ice behind. They set off this week on foot and by camels on a grueling trek across the burning Arabian desert.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In previous labs, we applied the function “nltk.word_tokenize” directly to multi-sentence text for simplicity.  But this function is actually trained to tokenize individual sentences and will work better if we first use the sentence splitter, aka tokenizer, to produce a list of text strings for individual sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Three Calgarians have found a rather unusual way of leaving snow and ice behind.',\n",
       " 'They set off this week on foot and by camels on a grueling trek across the burning Arabian desert.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textsplit = nltk.sent_tokenize(text)\n",
    "textsplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After producing the list of sentence texts, apply the word tokenizer to each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Three',\n",
       "  'Calgarians',\n",
       "  'have',\n",
       "  'found',\n",
       "  'a',\n",
       "  'rather',\n",
       "  'unusual',\n",
       "  'way',\n",
       "  'of',\n",
       "  'leaving',\n",
       "  'snow',\n",
       "  'and',\n",
       "  'ice',\n",
       "  'behind',\n",
       "  '.'],\n",
       " ['They',\n",
       "  'set',\n",
       "  'off',\n",
       "  'this',\n",
       "  'week',\n",
       "  'on',\n",
       "  'foot',\n",
       "  'and',\n",
       "  'by',\n",
       "  'camels',\n",
       "  'on',\n",
       "  'a',\n",
       "  'grueling',\n",
       "  'trek',\n",
       "  'across',\n",
       "  'the',\n",
       "  'burning',\n",
       "  'Arabian',\n",
       "  'desert',\n",
       "  '.']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokentext = [nltk.word_tokenize(sent) for sent in textsplit]\n",
    "tokentext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply the t2 bigram POS tagger to each sentence of tokens in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Three', 'NN'),\n",
       "  ('Calgarians', 'NN'),\n",
       "  ('have', 'NN'),\n",
       "  ('found', 'NN'),\n",
       "  ('a', 'DT'),\n",
       "  ('rather', 'NN'),\n",
       "  ('unusual', 'NN'),\n",
       "  ('way', 'NN'),\n",
       "  ('of', 'NN'),\n",
       "  ('leaving', 'NN'),\n",
       "  ('snow', 'NN'),\n",
       "  ('and', 'NN'),\n",
       "  ('ice', 'NN'),\n",
       "  ('behind', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('They', 'NN'),\n",
       "  ('set', 'NN'),\n",
       "  ('off', 'NN'),\n",
       "  ('this', 'NN'),\n",
       "  ('week', 'NN'),\n",
       "  ('on', 'NN'),\n",
       "  ('foot', 'NN'),\n",
       "  ('and', 'NN'),\n",
       "  ('by', 'NN'),\n",
       "  ('camels', 'NN'),\n",
       "  ('on', 'NN'),\n",
       "  ('a', 'DT'),\n",
       "  ('grueling', 'NN'),\n",
       "  ('trek', 'NN'),\n",
       "  ('across', 'NN'),\n",
       "  ('the', 'DT'),\n",
       "  ('burning', 'NN'),\n",
       "  ('Arabian', 'NN'),\n",
       "  ('desert', 'NN'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taggedtext = [t2.tag(tokens) for tokens in tokentext]\n",
    "taggedtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that this text has quite a few words that appear to be unknown to this tagger from the data it was trained on.  Examples of this are “Calgarians” and “camels”.  In both cases, these two words are tagged as NN instead of the correct tags of NNPS and NNS, respectively.  This points out the benefit of adding sequence information such as an HMM tagger would use and lexical information, such as a Maximum Entropy tagger could use if you defined such features.  In the NLTK, another strategy would be to use a Regular Expression tagger as a backoff tagger that could take into account word features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford POS Tagger\n",
    "\n",
    "One of the problems with training our own POS tagger is that we don’t have all the Penn Treebank data.  But NLTK also provides some taggers that come pre-trained on the larger amount of data.  One of these is the Stanford POS tagger, which was trained using a maximum entropy classifier.  This is described in the nltk.tag module:\n",
    "\n",
    "http://www.nltk.org/_modules/nltk/tag.html\n",
    "\n",
    "This tagger is available in the module: 'taggers/maxent_treebank_pos_tagger/english.pickle' and it is used for the standard nltk.pos_tag function.\n",
    "\n",
    "We use the standard nltk pos tagger on the same example text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Three', 'CD'),\n",
       "  ('Calgarians', 'NNPS'),\n",
       "  ('have', 'VBP'),\n",
       "  ('found', 'VBN'),\n",
       "  ('a', 'DT'),\n",
       "  ('rather', 'RB'),\n",
       "  ('unusual', 'JJ'),\n",
       "  ('way', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('leaving', 'VBG'),\n",
       "  ('snow', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('ice', 'NN'),\n",
       "  ('behind', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('They', 'PRP'),\n",
       "  ('set', 'VBD'),\n",
       "  ('off', 'RP'),\n",
       "  ('this', 'DT'),\n",
       "  ('week', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('foot', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('by', 'IN'),\n",
       "  ('camels', 'NNS'),\n",
       "  ('on', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('grueling', 'NN'),\n",
       "  ('trek', 'NN'),\n",
       "  ('across', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('burning', 'NN'),\n",
       "  ('Arabian', 'JJ'),\n",
       "  ('desert', 'NN'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taggedtextStanford = [nltk.pos_tag(tokens) for tokens in tokentext]\n",
    "taggedtextStanford\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we first split our text into a list of sentences and then each sentence into a list of tokens, our tagged text has the structure of a list of lists.  Suppose that instead we just want one long list of tagged tokens.  We can use a list comprehension to define the new list as all of the tagged tokens in each for the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Three', 'NN'), ('Calgarians', 'NN'), ('have', 'NN'), ('found', 'NN'), ('a', 'DT'), ('rather', 'NN'), ('unusual', 'NN'), ('way', 'NN'), ('of', 'NN'), ('leaving', 'NN'), ('snow', 'NN'), ('and', 'NN'), ('ice', 'NN'), ('behind', 'NN'), ('.', '.'), ('They', 'NN'), ('set', 'NN'), ('off', 'NN'), ('this', 'NN'), ('week', 'NN'), ('on', 'NN'), ('foot', 'NN'), ('and', 'NN'), ('by', 'NN'), ('camels', 'NN'), ('on', 'NN'), ('a', 'DT'), ('grueling', 'NN'), ('trek', 'NN'), ('across', 'NN'), ('the', 'DT'), ('burning', 'NN'), ('Arabian', 'NN'), ('desert', 'NN'), ('.', '.')]\n",
      "[('Three', 'CD'), ('Calgarians', 'NNPS'), ('have', 'VBP'), ('found', 'VBN'), ('a', 'DT'), ('rather', 'RB'), ('unusual', 'JJ'), ('way', 'NN'), ('of', 'IN'), ('leaving', 'VBG'), ('snow', 'NN'), ('and', 'CC'), ('ice', 'NN'), ('behind', 'NN'), ('.', '.'), ('They', 'PRP'), ('set', 'VBD'), ('off', 'RP'), ('this', 'DT'), ('week', 'NN'), ('on', 'IN'), ('foot', 'NN'), ('and', 'CC'), ('by', 'IN'), ('camels', 'NNS'), ('on', 'IN'), ('a', 'DT'), ('grueling', 'NN'), ('trek', 'NN'), ('across', 'IN'), ('the', 'DT'), ('burning', 'NN'), ('Arabian', 'JJ'), ('desert', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "taggedtext_flat = [pair for sent in taggedtext for pair in sent]\n",
    "print(taggedtext_flat)\n",
    "\n",
    "taggedtextStanford_flat = [pair for sent in taggedtextStanford for pair in sent]\n",
    "print(taggedtextStanford_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
