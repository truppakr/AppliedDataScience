{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Lab Session Week 9\n",
    "### More on Features and Evaluation for Classification\n",
    "### Part 1:  Bigram Features\n",
    "\n",
    "Getting Started\n",
    "\n",
    "For this lab session download the examples:  LabWk9.bigramsPOSeval.py and put it in your class folder for copy/pasting examples.  Start your jupyter notebook session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this week’s lab, we show two more types of features sometimes used in classification and how to use more classifier evaluation measures and methods.  After this week’s lab, you should be able to use a variety of features to test with your final project data, and also be able to report better evaluation measures with cross-validation.\n",
    "\n",
    "#### Bigram Features\n",
    "\n",
    "One more important source of features often used in sentiment and other document or sentence-level classifications is bigram features.  Typically these features are added to word level features.\n",
    "\n",
    "First, we restart by loading the movie review sentences and getting the baseline performance of the unigram features.  This is a repeat from last week in order to get started, except that we will change the size of the feature sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import sentence_polarity\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The movie review documents are not labeled individually, but are separated into file directories by category.  We first create the list of documents/sentences where each is paired with its label.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [(sent, cat) for cat in sentence_polarity.categories() \n",
    "    for sent in sentence_polarity.sents(categories=cat)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this list, each item is a pair (d,c) where d is a list of words from a sentence and c is its label, either ‘pos’ or ‘neg’.\n",
    "\n",
    "Since the documents are in order by label, we mix them up for later separation into training and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['the',\n",
       "   'movie',\n",
       "   'keeps',\n",
       "   'coming',\n",
       "   'back',\n",
       "   'to',\n",
       "   'the',\n",
       "   'achingly',\n",
       "   'unfunny',\n",
       "   'phonce',\n",
       "   'and',\n",
       "   'his',\n",
       "   'several',\n",
       "   'silly',\n",
       "   'subplots',\n",
       "   '.'],\n",
       "  'neg'),\n",
       " ([\"it's\",\n",
       "   'clear',\n",
       "   'the',\n",
       "   'filmmakers',\n",
       "   \"weren't\",\n",
       "   'sure',\n",
       "   'where',\n",
       "   'they',\n",
       "   'wanted',\n",
       "   'their',\n",
       "   'story',\n",
       "   'to',\n",
       "   'go',\n",
       "   ',',\n",
       "   'and',\n",
       "   'even',\n",
       "   'more',\n",
       "   'clear',\n",
       "   'that',\n",
       "   'they',\n",
       "   'lack',\n",
       "   'the',\n",
       "   'skills',\n",
       "   'to',\n",
       "   'get',\n",
       "   'us',\n",
       "   'to',\n",
       "   'this',\n",
       "   'undetermined',\n",
       "   'destination',\n",
       "   '.'],\n",
       "  'neg'),\n",
       " (['at',\n",
       "   'heart',\n",
       "   'the',\n",
       "   'movie',\n",
       "   'is',\n",
       "   'a',\n",
       "   'deftly',\n",
       "   'wrought',\n",
       "   'suspense',\n",
       "   'yarn',\n",
       "   'whose',\n",
       "   'richer',\n",
       "   'shadings',\n",
       "   'work',\n",
       "   'as',\n",
       "   'coloring',\n",
       "   'rather',\n",
       "   'than',\n",
       "   'substance',\n",
       "   '.'],\n",
       "  'pos'),\n",
       " (['falls',\n",
       "   'neatly',\n",
       "   'into',\n",
       "   'the',\n",
       "   'category',\n",
       "   'of',\n",
       "   'good',\n",
       "   'stupid',\n",
       "   'fun',\n",
       "   '.'],\n",
       "  'pos'),\n",
       " (['big',\n",
       "   'fat',\n",
       "   'liar',\n",
       "   'is',\n",
       "   'little',\n",
       "   'more',\n",
       "   'than',\n",
       "   'home',\n",
       "   'alone',\n",
       "   'raised',\n",
       "   'to',\n",
       "   'a',\n",
       "   'new',\n",
       "   ',',\n",
       "   'self-deprecating',\n",
       "   'level',\n",
       "   '.'],\n",
       "  'neg')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to define the set of words that will be used for features.  For this week’s lab, we will limit the length of the \n",
    "# word features to 1500.\n",
    "\n",
    "all_words_list = [word for (sent,cat) in documents for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "word_items = all_words.most_common(1500)\n",
    "word_features = [word for (word, freq) in word_items]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As before, the word feature labels will be ‘V_keyword)’ for each keyword (aka word) in the word_features set, \n",
    "# and the value of the feature will be Boolean,  according to whether the word is contained in that document.\n",
    "\n",
    "def document_features(document, word_features):\n",
    "\tdocument_words = set(document)\n",
    "\tfeatures = {}\n",
    "\tfor word in word_features:\n",
    "\t\tfeatures['V_{}'.format(word)] = (word in document_words)\n",
    "\treturn features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10662"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the feature sets for the documents. \n",
    "featuresets = [(document_features(d,word_features), c) for (d,c) in documents]\n",
    "len(featuresets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.742\n"
     ]
    }
   ],
   "source": [
    "# We create the training and test sets, train a Naïve Bayes classifier, and look at the accuracy.  \n",
    "# We separate the data into a 90%, 10% split for training and testing.\n",
    "\n",
    "train_set, test_set = featuresets[1000:], featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a baseline for performance for this random split of the data, we’ll create some bigram features.  \n",
    "\n",
    "As we saw in the lab in Week 3, when we worked on generating bigrams from documents, if we want to use highly frequent bigrams, we need to filter out special characters, which were very frequent in the bigrams, and also filter by frequency.  The bigram pmi measure also required some filtering to get frequent and meaningful bigrams.  \n",
    "\n",
    "But there is another bigram association measure that is more often used to filter bigrams for classification features.  This is the chi-squared measure, which is another measure of information gain, but which does its own frequency filtering.  Another frequently used alternative is to just use frequency, which is the bigram measure raw_freq.\n",
    "\n",
    "We’ll start by importing the collocations package and creating a short cut variable name for the bigram association measures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a bigram collocation finder using the original movie review words, since the bigram finder must have the words \n",
    "# in order.  Note that our all_words_list has exactly this list.\n",
    "\n",
    "all_words_list[:50]\n",
    "finder = BigramCollocationFinder.from_words(all_words_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the chi-squared measure to get bigrams that are informative features.  \n",
    "# Note that we don’t need to get the scores of the bigrams, so we use the nbest function which just returns the highest scoring \n",
    "# bigrams, using the number specified. (Or try bigram_measures.raw_freq.)\n",
    "\n",
    "bigram_features = finder.nbest(bigram_measures.chi_sq, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"''independent\", \"film''\"), (\"'60s-homage\", 'pokepie'), (\"'[the\", 'cockettes]'), (\"'ace\", \"ventura'\"), (\"'alternate\", \"reality'\"), (\"'aunque\", 'recurre'), (\"'black\", \"culture'\"), (\"'blue\", \"crush'\"), (\"'chan\", \"moment'\"), (\"'chick\", \"flicks'\"), (\"'date\", \"movie'\"), (\"'ethnic\", 'cleansing'), (\"'face\", \"value'\"), (\"'fully\", \"experienced'\"), (\"'jason\", \"x'\"), (\"'juvenile\", \"delinquent'\"), (\"'laugh\", \"therapy'\"), (\"'masterpiece\", \"theatre'\"), (\"'nicholas\", \"nickleby'\"), (\"'old\", \"neighborhood'\"), (\"'opening\", \"up'\"), (\"'rare\", \"birds'\"), (\"'sacre\", 'bleu'), (\"'science\", \"fiction'\"), (\"'shindler's\", \"list'\"), (\"'snow\", \"dogs'\"), (\"'some\", \"body'\"), (\"'special\", \"effects'\"), (\"'terrible\", \"filmmaking'\"), (\"'time\", \"waster'\"), (\"'true\", \"story'\"), (\"'unfaithful'\", 'cheats'), (\"'very\", \"sneaky'\"), (\"'we're\", '-doing-it-for'), (\"'who's\", \"who'\"), ('-after', 'spangle'), ('-as-it-', 'thinks-it-is'), ('-as-nasty', '-as-it-'), ('-doing-it-for', \"-the-cash'\"), ('10-course', 'banquet'), ('10-year', 'delay'), ('15-cent', 'stump'), ('18-year-old', 'mistress'), (\"1950's\", 'doris'), (\"1983's\", 'koyaanisqatsi'), ('1986', 'harlem'), (\"1988's\", 'powaqqatsi'), ('1992', 'malfitano-domingo'), (\"1992's\", 'unforgiven'), ('22-year-old', 'girlfriend')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Arthur', 'carefully'),\n",
       " ('carefully', 'rode'),\n",
       " ('rode', 'the'),\n",
       " ('the', 'brown'),\n",
       " ('brown', 'horse'),\n",
       " ('horse', 'around'),\n",
       " ('around', 'the'),\n",
       " ('the', 'castle')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The nbest function returns a list of significant bigrams in this corpus, and we can look at some of them.\n",
    "\n",
    "print(bigram_features[:50])\n",
    "\n",
    "# We are going to use these bigrams as features in a new features function.  In order to test if any bigram in the \n",
    "# bigram_features list is in the document, we need to generate the bigrams of the document, which we do using the \n",
    "# nltk.bigrams function.  To show this, we define a sentence and show the bigrams.\n",
    "\n",
    "sent = ['Arthur','carefully','rode','the','brown','horse','around','the','castle']\n",
    "sentbigrams = list(nltk.bigrams(sent))\n",
    "sentbigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "B_brown_horse\n"
     ]
    }
   ],
   "source": [
    "# For any one bigram, we can test if it is in the bigrams of the sentence and we can use string formatting, \n",
    "# with two occurrences of {}s, to insert the two words of the bigram into the name of the feature.\n",
    "\n",
    "bigram = ('brown','horse')\n",
    "print(bigram in sentbigrams)\n",
    "print('B_{}_{}'.format(bigram[0], bigram[1]))\n",
    "\n",
    "# Now we create a feature extraction function that has all the word features as before, but also has bigram features.\n",
    "\n",
    "def bigram_document_features(document, word_features, bigram_features):\n",
    "    document_words = set(document)\n",
    "    document_bigrams = nltk.bigrams(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    for bigram in bigram_features:\n",
    "        features['B_{}_{}'.format(bigram[0], bigram[1])] = (bigram in document_bigrams)    \n",
    "    return features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.742\n"
     ]
    }
   ],
   "source": [
    "# Now we create feature sets as before, but using this feature extraction function.\n",
    "\n",
    "bigram_featuresets = [(bigram_document_features(d,word_features,bigram_features), c) for (d,c) in documents]\n",
    "\n",
    "#There should be 2000 features:  1500 word features and 500 bigram features\n",
    "\n",
    "len(bigram_featuresets[0][0].keys())\n",
    "\n",
    "train_set, test_set = bigram_featuresets[1000:], bigram_featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "\n",
    "# So in my random training, test split, the bigrams did not improve the classification for this data.  \n",
    "# But there are many classification tasks for which bigrams are important.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Lab Session Week 9\n",
    "### More on Features and Evaluation for Classification\n",
    "### Part 2:  POS tag features\n",
    "\n",
    "#### Continuing our session with the movie review sentences\n",
    "\n",
    "#### POS tag features\n",
    "\n",
    "There are some classification tasks where part-of-speech tag features can have an effect.  In my experience, this is more likely for shorter units of classification, such as sentence level classification or shorter social media such as tweets.\n",
    "\n",
    "The most common way to use POS tagging information is to include counts of various types of word tags.  Here is an example feature function that counts nouns, verbs, adjectives and adverbs for features.  [Note that this function calls nltk.pos_tag every time that it is run and for repeated experiments, you could pre-compute the pos tags and save them for every document.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Arthur', 'carefully', 'rode', 'the', 'brown', 'horse', 'around', 'the', 'castle']\n",
      "[('Arthur', 'NNP'), ('carefully', 'RB'), ('rode', 'VBD'), ('the', 'DT'), ('brown', 'JJ'), ('horse', 'NN'), ('around', 'IN'), ('the', 'DT'), ('castle', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Observing the Stanford POS tagger, which is the default in NLTK, on a sentence:\n",
    "print(sent)\n",
    "print(nltk.pos_tag(sent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the definition of our new feature function, adding POS tag counts to the word features.\n",
    "\n",
    "def POS_features(document):\n",
    "    document_words = set(document)\n",
    "    tagged_words = nltk.pos_tag(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    numNoun = 0\n",
    "    numVerb = 0\n",
    "    numAdj = 0\n",
    "    numAdverb = 0\n",
    "    for (word, tag) in tagged_words:\n",
    "        if tag.startswith('N'): numNoun += 1\n",
    "        if tag.startswith('V'): numVerb += 1\n",
    "        if tag.startswith('J'): numAdj += 1\n",
    "        if tag.startswith('R'): numAdverb += 1\n",
    "    features['nouns'] = numNoun\n",
    "    features['verbs'] = numVerb\n",
    "    features['adjectives'] = numAdj\n",
    "    features['adverbs'] = numAdverb\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([\"it's\", 'hard', 'not', 'to', 'feel', \"you've\", 'just', 'watched', 'a', 'feature-length', 'video', 'game', 'with', 'some', 'really', 'heavy', 'back', 'story', '.'], 'neg')\n",
      "num nouns 3\n",
      "num verbs 2\n",
      "num adjectives 2\n",
      "num adverbs 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.738"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try out the POS features.\n",
    "POS_featuresets = [(POS_features(d), c) for (d, c) in documents]\n",
    "# number of features for document 0\n",
    "len(POS_featuresets[0][0].keys())\n",
    "\n",
    "# Show the first sentence in your (randomly shuffled) documents and look at its POS tag features.\n",
    "\n",
    "print(documents[0])\n",
    "# the pos tag features for this sentence\n",
    "print('num nouns', POS_featuresets[0][0]['nouns'])\n",
    "print('num verbs', POS_featuresets[0][0]['verbs'])\n",
    "print('num adjectives', POS_featuresets[0][0]['adjectives'])\n",
    "print('num adverbs', POS_featuresets[0][0]['adverbs'])\n",
    "\n",
    "# Now split into training and test and rerun the classifier.\n",
    "train_set, test_set = POS_featuresets[1000:], POS_featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)\n",
    "\n",
    "#This improved classification a small amount for my train/test split.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Lab Session Week 9\n",
    "### More on Features and Evaluation for Classification\n",
    "### Part 3:  The Evaluation Method of Cross-Validation\n",
    "\n",
    "#### Continuing our session with the movie review sentences\n",
    "\n",
    "#### Cross-Validation\n",
    "\n",
    "As a final topic in evaluation, we have discussed that our testing of the features on the movie reviews and movie review sentences data is often skewed by the random sample.  The remedy for this is to use different chunks of the data as the test set to repeatedly train a model and then average our performance over those models.\n",
    "\n",
    "This method is called cross-validation, or sometimes k-fold cross-validation.  In this method, we choose a number of folds, k, which is usually a small number like 5 or 10.  We first randomly partition the development data into k subsets, each approximately equal in size.  Then we train the classifier k times, where, at each iteration, we use each subset in turn as the test set and the others as a training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"5_fold_cv.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "NLTK does not have a built-in function for cross-validation, but we can program the process in a function that takes the number of folds and the feature sets, and iterates over training and testing a classifier.  This function only reports accuracy for each fold and for the overall average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1066 0\n"
     ]
    }
   ],
   "source": [
    "subset_size = len(featuresets)//10\n",
    "i=0\n",
    "print(subset_size,i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(num_folds, featuresets):\n",
    "    subset_size = len(featuresets)//num_folds\n",
    "    accuracy_list = []\n",
    "    # iterate over the folds\n",
    "    for i in range(num_folds):\n",
    "        test_this_round = featuresets[i*subset_size:][:subset_size]\n",
    "        train_this_round = featuresets[:i*subset_size]+featuresets[(i+1)*subset_size:]\n",
    "        # train using train_this_round\n",
    "        classifier = nltk.NaiveBayesClassifier.train(train_this_round)\n",
    "        # evaluate against test_this_round and save accuracy\n",
    "        accuracy_this_round = nltk.classify.accuracy(classifier, test_this_round)\n",
    "        print(i, accuracy_this_round)\n",
    "        accuracy_list.append(accuracy_this_round)\n",
    "    # find mean accuracy over all rounds\n",
    "    print('mean accuracy', sum(accuracy_list) / num_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7504690431519699\n",
      "1 0.7401500938086304\n",
      "2 0.7073170731707317\n",
      "3 0.7420262664165104\n",
      "4 0.7157598499061913\n",
      "5 0.7504690431519699\n",
      "6 0.7542213883677298\n",
      "7 0.7607879924953096\n",
      "8 0.7495309568480301\n",
      "9 0.7317073170731707\n",
      "mean accuracy 0.7402439024390245\n"
     ]
    }
   ],
   "source": [
    "# Run the cross-validation on our word feature sets with 10 folds.\n",
    "cross_validation(10, featuresets)\n",
    "\n",
    "# Instead of accuracy, we should have a cross-validation function to report precision and recall for each label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7373358348968105\n",
      "1 0.7523452157598499\n",
      "2 0.7476547842401501\n",
      "3 0.7410881801125704\n",
      "4 0.7157598499061913\n",
      "5 0.7532833020637899\n",
      "6 0.7307692307692307\n",
      "7 0.7485928705440901\n",
      "8 0.7223264540337712\n",
      "9 0.7392120075046904\n",
      "mean accuracy 0.7388367729831146\n"
     ]
    }
   ],
   "source": [
    "# Run the cross-validation on our word bigram feature sets with 10 folds.\n",
    "cross_validation(10, bigram_featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7382739212007504\n",
      "1 0.7476547842401501\n",
      "2 0.7467166979362101\n",
      "3 0.7354596622889306\n",
      "4 0.7176360225140713\n",
      "5 0.7504690431519699\n",
      "6 0.7317073170731707\n",
      "7 0.7514071294559099\n",
      "8 0.7195121951219512\n",
      "9 0.7335834896810507\n",
      "mean accuracy 0.7372420262664166\n"
     ]
    }
   ],
   "source": [
    "# Run the cross-validation on our word POS feature sets with 10 folds.\n",
    "cross_validation(10, POS_featuresets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Lab Session Week 9\n",
    "### More on Features and Evaluation for Classification\n",
    "### Part 4:  Evaluation Measures:  Precision, Recall and F1\n",
    "\n",
    "#### Continuing our session with the movie review sentences\n",
    "\n",
    "#### Other Evaluation Measures\n",
    "\n",
    "So far, we have been using simple accuracy for a performance evaluation measure of the predictive capability of the model that was learned from the training data.  But we can learn more by looking at the predictions for each of the labels in our classifier.\n",
    "\n",
    "We start by looking at the confusion matrix, which shows the results of a test for how many of the actual class labels (the gold standard labels) match with the predicted labels.  In this diagram the two labels are called “Yes” and “No”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"confusion_matrix.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the predicted class is the same as the actual class, we call those examples the true positives.  When the actual class was supposed to be Yes, but the predicted class was No, we call those examples the false negatives.  When the actual class is No, but the classifier incorrectly predicted Yes, we call those examples the false positives.  The true negatives are the remaining examples that were correctly predicted No.  The number of each of these types of examples in the test set is put into the confusion matrix.\n",
    "\n",
    "Note that the intuition for the terminology comes from the idea that we are trying to find all the examples where the class label is Yes, the positive examples.  The false positives represent the positives which were predicted Wrong, and the false negatives represent the positives that were Missed.  This idea originated in the Information Retrieval field where the Yes answers represented documents that were correctly retrieved as the result of a search.  \n",
    "\n",
    "In keeping with this intuition, two commonly used measures come from IR, where IR is only interested in the positive labels.\n",
    "\n",
    "recall = TP / ( TP + FP )   \t(the percentage of actual yes answers that are right)\n",
    "precision =  TP / ( TP + FN ) (the percentage of predicted yes answers that are right)\n",
    "\n",
    "These two measures are sometimes combined into a kind of average, the harmonic mean, called the F-measure, which in its simplest form is:\n",
    "\n",
    "F-measure = 2 * (recall * precision) / (recall + precision)\n",
    "\n",
    "In situations where we are equally interested in correctly predicting Yes and No, and the numbers of these are roughly equal, then we may compute precision and recall for both the positive and negative labels.  And we can also use the accuracy measure.\n",
    "\n",
    "accuracy = TP + TN / (TP + FP + FN + TN)    (percentage of correct Yes and No out\t\t\t\t\t\t\tof all text examples)\n",
    "\n",
    "\n",
    "In the NLTK, the confusion matrix is given by a function that takes two lists of labels for the test set.  NLTK calls the first list the reference list, which is all the correct/gold labels for the test set, and the second list is the test list, which is all the predicted labels in the test set.  These two lists are both in the order of the test set, so they can be compared to see which examples the classifier model agreed on or not.\n",
    "\n",
    "First we build the reference and test lists from the classifier on the test set, but we will call them the gold list and the predicted list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos', 'neg', 'pos', 'pos', 'pos', 'pos', 'pos', 'pos', 'neg', 'pos', 'neg', 'pos', 'neg', 'pos', 'neg', 'neg', 'neg', 'neg', 'pos', 'neg', 'neg', 'pos', 'neg', 'neg', 'neg', 'pos', 'pos', 'neg', 'neg']\n",
      "['neg', 'pos', 'neg', 'pos', 'pos', 'pos', 'neg', 'pos', 'neg', 'neg', 'pos', 'neg', 'pos', 'neg', 'pos', 'neg', 'neg', 'neg', 'pos', 'pos', 'neg', 'neg', 'pos', 'pos', 'neg', 'neg', 'pos', 'pos', 'neg', 'neg']\n"
     ]
    }
   ],
   "source": [
    "# First we build the reference and test lists from the classifier on the test set, but we will call them the gold list and \n",
    "#the predicted list.\n",
    "\n",
    "goldlist = []\n",
    "predictedlist = []\n",
    "for (features, label) in test_set:\n",
    "    \tgoldlist.append(label)\n",
    "    \tpredictedlist.append(classifier.classify(features))\n",
    "\n",
    "# We can look at the first 30 examples and think about whether the corresponding elements of the last match.\n",
    "\n",
    "print(goldlist[:30])\n",
    "print(predictedlist[:30])\n",
    "\n",
    "# Now we use the NLTK function to define the confusion matrix, and we print it out:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |      p      n |\n",
      "    |      o      e |\n",
      "    |      s      g |\n",
      "----+---------------+\n",
      "pos | <36.4%> 13.8% |\n",
      "neg |  12.4% <37.4%>|\n",
      "----+---------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = nltk.ConfusionMatrix(goldlist, predictedlist)\n",
    "print(cm.pretty_format(sort_by_count=True, show_percents=True, truncate=9))\n",
    "\n",
    " \n",
    "# (row = gold; col = predicted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our movie sentences classification task, we have two class labels:  ‘neg’ and ‘pos’ (instead of Yes and No).  If we consider the ‘pos’ class as the positive class and the ‘neg’ as the negative class, then this confusion matrix is reversed from our previous version, and there are 352 True Positives, 375 True Negatives, 125 False Positives and 148 False Negatives.  Since this classification task is symmetric with respect to the two classes, we can flip the terminology and consider the ‘neg’ class as positive and the ‘pos’ class as negative.  In that case, there are 375 True Positives, 352 True Negatives, 148 False Positives, and125 False Negatives.\n",
    "\n",
    "Since we are interested in both the ‘pos’ and ‘neg’ classes, we next want to compute precision, recall and F1 for each class.  There are NLTK functions to do this, but they require a lot of setup to get the input in the correct forms.\n",
    "\n",
    "Instead, I have written a function that takes the gold list and the predicted list, computes the True Positives, True Negatives, False Positives, False Negatives and then uses those to compute the other measures for each class.  I called this function eval_measures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute precision, recall and F1 for each label\n",
    "#  and for any number of labels\n",
    "# Input: list of gold labels, list of predicted labels (in same order)\n",
    "# Output:  prints precision, recall and F1 for each label\n",
    "def eval_measures(gold, predicted):\n",
    "    # get a list of labels\n",
    "    labels = list(set(gold))\n",
    "    # these lists have values for each label \n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    F1_list = []\n",
    "    for lab in labels:\n",
    "        # for each label, compare gold and predicted lists and compute values\n",
    "        TP = FP = FN = TN = 0\n",
    "        for i, val in enumerate(gold):\n",
    "            if val == lab and predicted[i] == lab:  TP += 1\n",
    "            if val == lab and predicted[i] != lab:  FN += 1\n",
    "            if val != lab and predicted[i] == lab:  FP += 1\n",
    "            if val != lab and predicted[i] != lab:  TN += 1\n",
    "        # use these to compute recall, precision, F1\n",
    "        recall = TP / (TP + FP)\n",
    "        precision = TP / (TP + FN)\n",
    "        recall_list.append(recall)\n",
    "        precision_list.append(precision)\n",
    "        F1_list.append( 2 * (recall * precision) / (recall + precision))\n",
    "\n",
    "    # the evaluation measures in a table with one row per label\n",
    "    print('\\tPrecision\\tRecall\\t\\tF1')\n",
    "    # print measures for each label\n",
    "    for i, lab in enumerate(labels):\n",
    "        print(lab, '\\t', \"{:10.3f}\".format(precision_list[i]), \\\n",
    "          \"{:10.3f}\".format(recall_list[i]), \"{:10.3f}\".format(F1_list[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tPrecision\tRecall\t\tF1\n",
      "pos \t      0.725      0.746      0.735\n",
      "neg \t      0.751      0.730      0.741\n"
     ]
    }
   ],
   "source": [
    "# Now we can call this function on our data.\n",
    "\n",
    "eval_measures(goldlist, predictedlist)\n",
    "\n",
    "# This gives us more information into the performance of the model for each label.  \n",
    "# We can see that the ‘neg’ label is predicted with higher precision, .75,\n",
    "# while the ‘pos’ label is predicted with higher recall, .738.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
