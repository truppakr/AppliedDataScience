---
title: "ThulasiRam_RuppaKrishnan_HW5"
author: "Thulasiram Ruppa Krishnan"
date: "May 7, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load libraries
```{r}
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering visualization
library(dendextend) # for comparing two dendrograms
library(RWeka)      # for RWeka
library(tm)         # for Term Document Matrix
library(wordcloud)  # for wordclouds
library(tidytext)   # for AFFN, convert DTM to DF
library(stringr)
library(stringi)
# ONCE: install.packages("Snowball")
## NOTE Snowball is not yet available for R v 3.5.x
## So I cannot use it  - yet...
##library("Snowball")
##set working directory
## ONCE: install.packages("slam")
library(slam)
library(quanteda)
## ONCE: install.packages("quanteda")
## Note - this includes SnowballC
library(SnowballC)
library(arules)
##ONCE: install.packages('proxy')
library(proxy)
library(Matrix)
library(plyr) ## for adply
library(ggplot2)
library(mclust) # for Mclust EM clustering
library(knitr)
#install.packages("lda")
library(lda); library(reshape2)
# install.packages("rpart")
# install.packages('rattle')
# install.packages('rpart.plot')
# install.packages('RColorBrewer')
# install.packages("Cairo")
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(Cairo)
library(scales)
#install.packages("psych")
library(psych)
```


## Clear workspace
```{r}
# Clear objects
rm(list=ls())
## Set your working director to the path were your code AND datafile is
setwd("~/01 Personal/MS/IST 707/week5/temp/txt")
getwd()
#setwd("C://Users//rkrishnan//Documents//01 Personal//MS//IST 707//week4//temp")

```

## Load Corpus
```{r}
# 1) Using normalization (weighting, TF-IDF, z values, etc) should be done with care and should not create NA values) - however  - all kinds of strange things can happen in code.
# If you normalize, really think about what you are trying to accomplish and how you can make it work. 
# Then, do it on a very small example dataset that you built - to see if it does what you think it should.
# 
# 2) Text data will always have huge dimensional. 
# Methods to reduce:
# (a) Remove all words smaller than 2 (or 3).
# (b) Remove words larger than a certain length (this depends on many things)
# (c) Make everything lowercase.
# (d) Use stemming (so that hike, hikers, hiking, hiked, etc are all the same word). I coded this myself as I do not like the packaged stemmers. I shared this code.
# (e) Remove all words that contain numbers
# (f) Remove all punctuation
# (g) Remove stopwords - here you will have to think about what and how.
EssayCorpus <- Corpus(DirSource("C:/Users/rkrishnan/Documents/01 Personal/MS/IST 707/week5/temp/txt",pattern = ".txt"))
(getTransformations())
(ndocs<-length(EssayCorpus))

##The following will show you that you read in all the documents
(summary(EssayCorpus))
(meta(EssayCorpus[[1]]))
(meta(EssayCorpus[[1]],5))

# ignore extremely rare words i.e. terms that appear in less then 1% of the documents
(minTermFreq <- ndocs * 0.0001)
# ignore overly common words i.e. terms that appear in more than 50% of the documents
(maxTermFreq <- ndocs * 1)
(MyStopwords <- c("will", "shall", "may", "might", "can", "must","much","upon","shall"))
  #stopwords))
(STOPS <-stopwords('english'))
Essay_dtm <- DocumentTermMatrix(EssayCorpus,
                         control = list(
                           stopwords = TRUE, 
                           wordLengths=c(3, 15),
                           removePunctuation = T,
                           removeNumbers = T,
                           tolower=T,
                           stemming = T,
                           remove_separators = T,
                           stopwords = MyStopwords,
                           bounds = list(global = c(minTermFreq, maxTermFreq))
                         ))


## Have a look
tm::inspect(Essay_dtm)
DTM_mat <- as.matrix(Essay_dtm)
(DTM_mat[1:74,1:5])
str(DTM_mat)

## Normalize the data
DTM_mat_N <-apply(DTM_mat, 1, function(i) round(i/sum(i),3))
DTM_mat_N[1:5,1:5]

## Normalization will create a transpose of the orignal data , transpose it back to original form
Essay.dtm.m.n <- t(DTM_mat_N)
Essay.dtm.m.n[1:5,1:5]

str(Essay.dtm.m.n)
(Essay.dtm.m.n[,1:2])


```

## create training set and test set
```{r}
# Dispute Essays
(Essay.dtm.m.n[1:11,1:2])
# Hamilton Essays
(Essay.dtm.m.n[12:62,1:2])
# Madisson Essays
(Essay.dtm.m.n[71:85,1:2])

Essay.dtm.df.train <- data.frame(Essay.dtm.m.n[c(12:62,71:85),])
Essay.dtm.df.test <- data.frame(Essay.dtm.m.n[c(1:11),])

Essay.dtm.df.train <-tibble::rownames_to_column(Essay.dtm.df.train, "auth.h.m")
Essay.dtm.df.train$auth.h.m <- ifelse(substr(Essay.dtm.df.train$auth.h.m,1,1)=="H",1,0)


# Hamilton Essays
(Essay.dtm.df.train[1:51,1:5])
# Madisson Essays
(Essay.dtm.df.train[52:length(Essay.dtm.df.train[,1]),1:2])
#Dispute Essays
(Essay.dtm.df.test[1:11,1:2])

dim(Essay.dtm.df.train)
dim(Essay.dtm.df.test)

# Make sure label is a factor
Essay.dtm.df.train$auth.h.m <- as.factor(Essay.dtm.df.train$auth.h.m)

str(Essay.dtm.df.train)
str(Essay.dtm.df.test)

```

# create model with all variables
```{r}

fit1.rpart <- rpart(auth.h.m ~ ., data = Essay.dtm.df.train, method = "class"
                    #,control = rpart.control(minsplit = 1, cp = 0.2)
                    )
summary(fit1.rpart)
predicted1.rpart=predict(fit1.rpart,Essay.dtm.df.test, type="class")
(head(predicted1.rpart,n=10))
#(head(test, n=10))
plot(fit1.rpart)
text(fit1.rpart)
fancyRpartPlot(fit1.rpart)

fit2.j48=J48(auth.h.m~., data = Essay.dtm.df.train, control=Weka_control(U=FALSE, M=2, C=0.1))
e <- evaluate_Weka_classifier(fit2.j48, numFolds = 10, seed = 1, class = TRUE)
predicted2.j48=predict (fit2.j48, newdata = Essay.dtm.df.test, type = c("class"))
InfoGainAttributeEval(auth.h.m ~ . , data = Essay.dtm.df.train)
(fit2.j48)


```

# Create separate Hamilton and Madison Essays
```{r}

# Hamilton Essays
(Essay.dtm.df.train[1:51,1:5])
h.col.csums <-data.frame(top=sort(colSums(Essay.dtm.df.train[1:51,2:length(Essay.dtm.df.train[1,])]),decreasing = TRUE)[1:(length(Essay.dtm.df.train[1,])*0.2)])
h.col.csums.nz <-data.frame(top=sort(colSums(Essay.dtm.df.train[1:51,2:length(Essay.dtm.df.train[1,])]),decreasing = TRUE))
h.col.csums.nz<-tibble::rownames_to_column(h.col.csums.nz, "col")
h.col.csums.nz <-h.col.csums.nz[(which(h.col.csums.nz$top!=0)),]
Essay.dtm.df.testl <- cbind(auth.h.m=2,Essay.dtm.df.test)

# Madisson Essays
(Essay.dtm.df.train[52:length(Essay.dtm.df.train[,1]),1:2])
m.col.csums <-data.frame(top=sort(colSums(Essay.dtm.df.train[52:length(Essay.dtm.df.train[,1]),2:length(Essay.dtm.df.train[1,])]),decreasing = TRUE)[1:(length(Essay.dtm.df.train[1,])*0.2)])
m.col.csums.nz <-data.frame(top=sort(colSums(Essay.dtm.df.train[52:length(Essay.dtm.df.train[,1]),2:length(Essay.dtm.df.train[1,])]),decreasing = TRUE))
m.col.csums.nz<-tibble::rownames_to_column(m.col.csums.nz, "col")
m.col.csums.nz <-m.col.csums.nz[(which(m.col.csums.nz$top!=0)),]

hrn<-data.frame(col=row.names(h.col.csums))
mrn<-data.frame(col=row.names(m.col.csums))
id.col<-data.frame(col="auth.h.m")

pick.col.names<-unique(rbind(id.col,hrn,mrn))

hmrn <-data.frame(col=intersect(h.col.csums.nz$col, m.col.csums.nz$col))
pick.col.names2<-unique(rbind(id.col,hmrn))
length(hmrn)
# create training dataset with top columns
Essay.dtm.df.combined <-rbind(Essay.dtm.df.train,Essay.dtm.df.testl)
Essay.dtm.df.train.cols <- (Essay.dtm.df.combined[1:66,c(pick.col.names$col)])
Essay.dtm.df.test.cols <-(Essay.dtm.df.combined[67:77,c(pick.col.names$col)])

Essay.dtm.df.train.cols.nz <- (Essay.dtm.df.combined[1:66,c(pick.col.names2$col)])
Essay.dtm.df.test.cols.nz <-(Essay.dtm.df.combined[67:77,c(pick.col.names2$col)])

write.csv(Essay.dtm.df.train.cols,"train.csv")
write.csv(Essay.dtm.df.test.cols,"test.csv")
write.csv(Essay.dtm.df.train[1:51,c(hmrn$col)],"hmrn.csv")

# im.h <-Essay.dtm.df.train[1:51,c(hmrn$col)]
# 
# h.col.csums.it <-data.frame(top=sort(colSums(im.h),decreasing = TRUE)[1:100])
# im.m <-Essay.dtm.df.train[52:length(Essay.dtm.df.train[,1]),c(hmrn$col)]
# m.col.csums.it <-data.frame(top=sort(colSums(im.m),decreasing = TRUE)[1:100])

```


# create model with top 20% variables
```{r}
str(Essay.dtm.df.train.cols)
str(Essay.dtm.df.test.cols)
fit5.rpart <- rpart(auth.h.m ~ ., data = Essay.dtm.df.train.cols, method = "class"
                    ,control = rpart.control(minsplit = 10, cp = 0.2)
                    )
summary(fit5.rpart)
predicted5.rpart=predict(fit5.rpart,Essay.dtm.df.test.cols[,-1], type="class")
(head(predicted5.rpart,n=10))
#(head(test, n=10))
plot(fit5.rpart)
text(fit5.rpart)
fancyRpartPlot(fit5.rpart)

fit6.j48=J48(auth.h.m~., data = Essay.dtm.df.train.cols, control=Weka_control(U=FALSE, M=15, C=0.1))
e <- evaluate_Weka_classifier(fit6.j48, numFolds = 10, seed = 1, class = TRUE)
predicted6.j48=predict (fit6.j48, newdata = Essay.dtm.df.test.cols[,-1], type = c("class"))
InfoGainAttributeEval(auth.h.m ~ . , data = Essay.dtm.df.train.cols)
(fit6.j48)


```

#create model with non zero variables
```{r}


fit7.rpart <- rpart(auth.h.m ~ ., data = Essay.dtm.df.train.cols.nz, method = "class"
                    ,control = rpart.control(minsplit = 10, cp = 0.2)
                    )
summary(fit7.rpart)
predicted7.rpart=predict(fit7.rpart,Essay.dtm.df.test.cols.nz[,-1], type="class")
(head(predicted7.rpart,n=10))
#(head(test, n=10))
plot(fit7.rpart)
text(fit7.rpart)
fancyRpartPlot(fit7.rpart)

fit8.j48=J48(auth.h.m~., data = Essay.dtm.df.train.cols.nz, control=Weka_control(U=FALSE, M=15, C=0.1))
e <- evaluate_Weka_classifier(fit8.j48, numFolds = 10, seed = 1, class = TRUE)
predicted8.j48=predict (fit8.j48, newdata = Essay.dtm.df.test.cols.nz[,-1], type = c("class"))
InfoGainAttributeEval(auth.h.m ~ . , data = Essay.dtm.df.test.cols.nz)
(fit8.j48)


```

## part of speech
```{r}
# install.packages("devtools")
# library(devtools)
# devtools::install_github("bnosac/RDRPOSTagger")
# library("RDRPOSTagger")
# devtools::install_github("ropensci/tokenizers")
# install.packages("data.table")
# install.packages("Rtools")
# 
# ## Rtools not working and hence trying with pkgbuild
# install.packages("pkgbuild")
# devtools::install_github("r-lib/pkgbuild")
# library(pkgbuild) # load package
# find_rtools() # should be TRUE, assuming you have Rtools 3.5
# # Set path of Rtools Sys.setenv(PATH = paste(Sys.getenv("PATH"), "*InstallDirectory*/Rtools/bin/", "*InstallDirectory*/Rtools/mingw_64/bin", sep = ";")) #for 64 bit version Sys.setenv(BINPREF = "*InstallDirectory*/Rtools/mingw_64/bin") library(devtools) #Manually "force" version to be accepted assignInNamespace("version_info", c(devtools:::version_info, list("3.5" = list(version_min = "3.3.0", version_max = "99.99.99", path = "bin"))), "devtools") find_rtools() # is TRUE now #
# 
# Sys.setenv(PATH = paste(Sys.getenv("PATH"), "*InstallDirectory*/Rtools/bin/","*InstallDirectory*/Rtools/mingw_64/bin", sep = ";"))
# 
# Sys.setenv(BINPREF = "*InstallDirectory*/Rtools/mingw_64/bin")
# library(devtools)
```

## Get the sentimaent score
## Load Positive/Negative Keywords and sentiments
```{r Load Positive/Negative Keywords}

pos <- "C:/Users/rkrishnan/Documents/01 Personal/MS/IST 687/opinion-lexicon-English/positive-words.txt"
neg <- "C:/Users/rkrishnan/Documents/01 Personal/MS/IST 687/opinion-lexicon-English/negative-words.txt" 

# read the files
p <- scan(pos,character(0),sep = "\n") 
n <- scan(neg,character(0),sep = "\n")

#remove the 1st 34 lines (Header Info)

p <- p[-1:-34]
n <- n[-1:-34]

head(p,10)
head(n,10)

sentiments
affin <- get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")

```

```{r Function to compute sentiment Score}


fnGetSentimentScore <- function(words,i){
  
matched <-match(words,affin$word,nomatch=0)
#print(paste("Matched :" ,matched))


wordCounts[which(matched !=0)]
affin$word[matched[which(matched !=0)]]
affin$score[matched[which(matched !=0)]]
mScore <- affin$score[matched[which(matched !=0)]]



pScore.m[i] <<- sum(ifelse(mScore >0, mScore, 0))
nScore.m[i] <<- abs(sum(ifelse(mScore <0, mScore, 0)))
totalScore.m[i] <<- sum(abs(mScore))

print(paste(i," - Essay" ))
print(paste("_________________________________" ))
# Overall Score 
print(paste("Total Score :" ,totalScore.m[i]))
print(paste("Positive Score :" ,pScore.m[i]))
print(paste("Negative Score :", nScore.m[i]))


#ratio of  postive and negative  Score

ratioPosScore.m[i] <<-pScore.m[i]/totalScore.m[i]
ratioNegScore.m[i] <<-nScore.m[i]/totalScore.m[i]


print(paste("Positive Score ratio :" ,ratioPosScore.m[i]))
print(paste("Negative Score ratio :" ,ratioNegScore.m[i]))

print(paste("_________________________________" ))

}


fnGetSentimentScore2 <- function(words,i){
  
matched <-match(words,affin$word,nomatch=0)
#print(paste("Matched :" ,matched))


wordCounts[which(matched !=0)]
affin$word[matched[which(matched !=0)]]
affin$score[matched[which(matched !=0)]]
mScore <- affin$score[matched[which(matched !=0)]]

pScore <- sum(ifelse(mScore >0, mScore, 0))
nScore <- abs(sum(ifelse(mScore <0, mScore, 0)))
totalScore <- sum(abs(mScore))

print(paste(i," - 25% of the speech" ))
# Overall Score 
totalScore
print(paste("Total Score :" ,totalScore))
pScore
print(paste("Positive Score :" ,pScore))
nScore
print(paste("Negative Score :", nScore))


#ratio of  postive and negative  Score

ratioPosScore <-pScore/totalScore
ratioNegScore <-nScore/totalScore

ratioPosScore
print(paste("Positive Score ratio :" ,ratioPosScore))
ratioNegScore
print(paste("Negative Score ratio :" ,ratioNegScore))



}

```

## Load Hamilton Essays
```{r Load Hamilton Essays}

pScore.m <- 0
nScore.m <- 0
totalScore.m <- 0
ratioPosScore.m <- 0
ratioNegScore.m <- 0
file_nm <-""
Hamilton.files <- list.files("C:/Users/rkrishnan/Documents/01 Personal/MS/IST 707/week5/temp/txt/", pattern="Hamilton", all.files=FALSE,full.names=FALSE)

 for(i in 1:length(Hamilton.files)){
  
sbaFile <- paste("C:/Users/rkrishnan/Documents/01 Personal/MS/IST 707/week5/temp/txt/",Hamilton.files[i],sep = "")

sba <- readLines(sbaFile)
str(sba)

# Text Transformation
words.vec <-VectorSource(sba)
words.corpus <-Corpus(words.vec)
words.corpus
#inspect(words.corpus)

words.corpus <- tm_map(words.corpus,content_transformer(tolower))
words.corpus <- tm_map(words.corpus,removePunctuation)
words.corpus <- tm_map(words.corpus,removeNumbers)
words.corpus <- tm_map(words.corpus,removeWords,stopwords("english"))

## Create Term Document Matrix

tdm <-TermDocumentMatrix(words.corpus)
(tdm)
m <-as.matrix(tdm)
wordCounts <- rowSums(m)
wordCounts <- sort(wordCounts,decreasing = TRUE)
head(wordCounts)
cloudFrame <-data.frame(word=names(wordCounts),freq=wordCounts)
#wordcloud(cloudFrame$word,cloudFrame$freq)

## Plot Word Cloud
# wordcloud(names(wordCounts),wordCounts,min.freq = 2,max.words = 50,rot.per = 0.35,colors = brewer.pal(8,"Dark2"))

## Sentiment Analysis


#calculate the total number of words
totalwords <- sum(wordCounts)

#have a vector that just has all the words
words <-names(wordCounts)
matched <- match(words,p,nomatch=0)

## For Testing positive words

matched.df <-data.frame(matched.id=as.character(seq_len(length(matched))),matched=matched)
matched.df <- matched.df[which(matched.df$matched!=0),]
paste(p[as.numeric(as.character(matched.df[1,2]))],"=",words[as.numeric(as.character(matched.df[1,1]))])
  
# Store Positive Counts
mCounts <-wordCounts[which(matched !=0)]
length(mCounts)

mWords <- names(mCounts)
nPos <- sum(mCounts)
nPos


matched <- match(words,n,nomatch=0)

## For Testing negative words
matched.df <-data.frame(matched.id=as.character(seq_len(length(matched))),matched=matched)
matched.df <- matched.df[which(matched.df$matched!=0),]
paste(n[as.numeric(as.character(matched.df[1,2]))],"=",words[as.numeric(as.character(matched.df[1,1]))])

# Store Negative Counts
nCounts <-wordCounts[which(matched !=0)]
length(nCounts)

nWords <- names(nCounts)
nNeg <- sum(nCounts)
nNeg

## Calculate the Sentiment

# calculate the % of words that are positive and negative
totalWords <-length(words)

ratioPos <-nPos/totalWords
ratioPos

ratioNeg <-nNeg/totalWords
ratioNeg


# compute the overall score using AFFIN word list

matched <-match(words,affin$word,nomatch=0)

## For Testing affinity match
matched.df <-data.frame(matched.id=as.character(seq_len(length(matched))),matched=matched)
matched.df <- matched.df[which(matched.df$matched!=0),]
paste(affin$word[as.numeric(as.character(matched.df[1,2]))],"=",words[as.numeric(as.character(matched.df[1,1]))], ", Score: ",affin$score[as.numeric(as.character(matched.df[1,2]))])


wordCounts[which(matched !=0)]
affin$word[matched[which(matched !=0)]]
affin$score[matched[which(matched !=0)]]
mScore <- affin$score[matched[which(matched !=0)]]

pScore <- sum(ifelse(mScore >0, mScore, 0))
nScore <- abs(sum(ifelse(mScore <0, mScore, 0)))
totalScore <- sum(abs(mScore))

# Overall Score 
totalScore
pScore
nScore


#ratio of  postive and negative  Score

ratioPosScore <-pScore/totalScore
ratioNegScore <-nScore/totalScore

ratioPosScore
ratioNegScore

fnGetSentimentScore(words,i)
file_nm[i] <-Hamilton.files[i]
}

## r Plot Sentiment Score

Hamilton.essay.sentiment.score <- data.frame(cbind(c(1:length(Hamilton.files)),file_nm,as.numeric(as.character(totalScore.m)),as.numeric(as.character(pScore.m)),as.numeric(as.character(nScore.m)),as.numeric(as.character(ratioPosScore.m)),as.numeric(as.character(ratioNegScore.m))))
Hamilton.essay.sentiment.score <-`colnames<-`(Hamilton.essay.sentiment.score,c("Essay","File_nm","Total_Score","Positive_Score","Negative_Score","Positive_Ratio","Negative_Ratio"))


# convert score to numeric data
Hamilton.essay.sentiment.score$Total_Score <- as.numeric(as.character(Hamilton.essay.sentiment.score$Total_Score))
Hamilton.essay.sentiment.score$Positive_Score <- as.numeric(as.character(Hamilton.essay.sentiment.score$Positive_Score))
Hamilton.essay.sentiment.score$Negative_Score <- as.numeric(as.character(Hamilton.essay.sentiment.score$Negative_Score))


options(digits = 4)
Hamilton.essay.sentiment.score$Positive_Ratio <- as.numeric(as.character(Hamilton.essay.sentiment.score$Positive_Ratio))
Hamilton.essay.sentiment.score$Negative_Ratio <- as.numeric(as.character(Hamilton.essay.sentiment.score$Negative_Ratio))

Hamilton.essay.sentiment.score$auth.h.m <- 1


ggplot() + geom_bar(data = Hamilton.essay.sentiment.score,aes(x=Essay,y=Total_Score),stat="identity")+labs (x="Essay Number",y="Total Sentiment Score",title = "Total Sentiment Score by  Essay") + theme(legend.position = "bottom") 

ggplot() + geom_bar(data = Hamilton.essay.sentiment.score,aes(x=Essay,y=Positive_Score),stat="identity")+labs (x="Essay Number",y="Positive Sentiment Score",title = "Positive Sentiment Score by  Essay") + theme(legend.position = "bottom") 

ggplot() + geom_bar(data = Hamilton.essay.sentiment.score,aes(x=Essay,y=Negative_Score),stat="identity")+labs (x="Essay Number",y="Negative Sentiment Score",title = "Negative Sentiment Score by  Essay") + theme(legend.position = "bottom") 

ggplot() + geom_bar(data = Hamilton.essay.sentiment.score,aes(x=Essay,y=Positive_Ratio),stat="identity")+labs (x="Essay Number",y="Positive Sentiment Ratio",title = "Positive Sentiment Ratio by  Essay") + theme(legend.position = "bottom") 

ggplot() + geom_bar(data = Hamilton.essay.sentiment.score,aes(x=Essay,y=Negative_Ratio),stat="identity")+labs (x="Essay Number",y="Negative Sentiment Ratio",title = "Negative Sentiment Ratio by  Essay") + theme(legend.position = "bottom") 

ggplot() + geom_point(data = Hamilton.essay.sentiment.score,aes(x=Positive_Ratio,y=Negative_Ratio,size=Total_Score)) +labs (x="Positive Sentiment Ratio",y="Negative Sentiment Ratio",title = "Positive Vs Negative Sentiment Ratio for Hamilton Essays") + theme(legend.position = "bottom") 

```




## Load Madison Essays
```{r Load Madison Essays}


Madison.files <- list.files("C:/Users/rkrishnan/Documents/01 Personal/MS/IST 707/week5/temp/txt/", pattern="Madison", all.files=FALSE,full.names=FALSE)

 for(i in 1:length(Madison.files)){
  
sbaFile <- paste("C:/Users/rkrishnan/Documents/01 Personal/MS/IST 707/week5/temp/txt/",Madison.files[i],sep = "")

sba <- readLines(sbaFile)
str(sba)

# Text Transformation
words.vec <-VectorSource(sba)
words.corpus <-Corpus(words.vec)
words.corpus

words.corpus <- tm_map(words.corpus,content_transformer(tolower))
words.corpus <- tm_map(words.corpus,removePunctuation)
words.corpus <- tm_map(words.corpus,removeNumbers)
words.corpus <- tm_map(words.corpus,removeWords,stopwords("english"))

## Create Term Document Matrix

tdm <-TermDocumentMatrix(words.corpus)
(tdm)
m <-as.matrix(tdm)
wordCounts <- rowSums(m)
wordCounts <- sort(wordCounts,decreasing = TRUE)
head(wordCounts)
cloudFrame <-data.frame(word=names(wordCounts),freq=wordCounts)
#wordcloud(cloudFrame$word,cloudFrame$freq)

## Plot Word Cloud
# wordcloud(names(wordCounts),wordCounts,min.freq = 2,max.words = 50,rot.per = 0.35,colors = brewer.pal(8,"Dark2"))

## Sentiment Analysis


#calculate the total number of words
totalwords <- sum(wordCounts)

#have a vector that just has all the words
words <-names(wordCounts)
matched <- match(words,p,nomatch=0)

## For Testing positive words

matched.df <-data.frame(matched.id=as.character(seq_len(length(matched))),matched=matched)
matched.df <- matched.df[which(matched.df$matched!=0),]
paste(p[as.numeric(as.character(matched.df[1,2]))],"=",words[as.numeric(as.character(matched.df[1,1]))])
  
# Store Positive Counts
mCounts <-wordCounts[which(matched !=0)]
length(mCounts)

mWords <- names(mCounts)
nPos <- sum(mCounts)
nPos


matched <- match(words,n,nomatch=0)

## For Testing negative words
matched.df <-data.frame(matched.id=as.character(seq_len(length(matched))),matched=matched)
matched.df <- matched.df[which(matched.df$matched!=0),]
paste(n[as.numeric(as.character(matched.df[1,2]))],"=",words[as.numeric(as.character(matched.df[1,1]))])

# Store Negative Counts
nCounts <-wordCounts[which(matched !=0)]
length(nCounts)

nWords <- names(nCounts)
nNeg <- sum(nCounts)
nNeg

## Calculate the Sentiment

# calculate the % of words that are positive and negative
totalWords <-length(words)

ratioPos <-nPos/totalWords
ratioPos

ratioNeg <-nNeg/totalWords
ratioNeg


# compute the overall score using AFFIN word list

matched <-match(words,affin$word,nomatch=0)

## For Testing affinity match
matched.df <-data.frame(matched.id=as.character(seq_len(length(matched))),matched=matched)
matched.df <- matched.df[which(matched.df$matched!=0),]
paste(affin$word[as.numeric(as.character(matched.df[1,2]))],"=",words[as.numeric(as.character(matched.df[1,1]))], ", Score: ",affin$score[as.numeric(as.character(matched.df[1,2]))])


wordCounts[which(matched !=0)]
affin$word[matched[which(matched !=0)]]
affin$score[matched[which(matched !=0)]]
mScore <- affin$score[matched[which(matched !=0)]]

pScore <- sum(ifelse(mScore >0, mScore, 0))
nScore <- abs(sum(ifelse(mScore <0, mScore, 0)))
totalScore <- sum(abs(mScore))

# Overall Score 
totalScore
pScore
nScore


#ratio of  postive and negative  Score

ratioPosScore <-pScore/totalScore
ratioNegScore <-nScore/totalScore

ratioPosScore
ratioNegScore

fnGetSentimentScore(words,i)
file_nm[i] <-Madison.files[i]
}

## r Plot Sentiment Score

Madison.essay.sentiment.score <- data.frame(cbind(c(1:length(Madison.files)),file_nm[1:length(Madison.files)],totalScore.m[1:length(Madison.files)],pScore.m[1:length(Madison.files)],nScore.m[1:length(Madison.files)],ratioPosScore.m[1:length(Madison.files)],ratioNegScore.m[1:length(Madison.files)]))
Madison.essay.sentiment.score <-`colnames<-`(Madison.essay.sentiment.score,c("Essay","File_nm","Total_Score","Positive_Score","Negative_Score","Positive_Ratio","Negative_Ratio"))

# convert score to numeric data
Madison.essay.sentiment.score$Total_Score <- as.numeric(as.character(Madison.essay.sentiment.score$Total_Score))
Madison.essay.sentiment.score$Positive_Score <- as.numeric(as.character(Madison.essay.sentiment.score$Positive_Score))
Madison.essay.sentiment.score$Negative_Score <- as.numeric(as.character(Madison.essay.sentiment.score$Negative_Score))


options(digits = 4)
Madison.essay.sentiment.score$Positive_Ratio <- as.numeric(as.character(Madison.essay.sentiment.score$Positive_Ratio))
Madison.essay.sentiment.score$Negative_Ratio <- as.numeric(as.character(Madison.essay.sentiment.score$Negative_Ratio))

Madison.essay.sentiment.score$auth.h.m <- 0
ggplot() + geom_bar(data = Madison.essay.sentiment.score,aes(x=Essay,y=Total_Score),stat="identity")+labs (x="Essay Number",y="Total Sentiment Score",title = "Total Sentiment Score by  Essay") + theme(legend.position = "bottom") 

ggplot() + geom_bar(data = Madison.essay.sentiment.score,aes(x=Essay,y=Positive_Score),stat="identity")+labs (x="Essay Number",y="Positive Sentiment Score",title = "Positive Sentiment Score by Essay") + theme(legend.position = "bottom") 

ggplot() + geom_bar(data = Madison.essay.sentiment.score,aes(x=Essay,y=Negative_Score),stat="identity")+labs (x="Essay Number",y="Negative Sentiment Score",title = "Negative Sentiment Score by Essay") + theme(legend.position = "bottom") 

ggplot() + geom_bar(data = Madison.essay.sentiment.score,aes(x=Essay,y=Positive_Ratio),stat="identity")+labs (x="Essay Number",y="Positive Sentiment Ratio",title = "Positive Sentiment Ratio by Essay") + theme(legend.position = "bottom") 

ggplot() + geom_bar(data = Madison.essay.sentiment.score,aes(x=Essay,y=Negative_Ratio),stat="identity")+labs (x="Essay Number",y="Negative Sentiment Ratio",title = "Negative Sentiment Ratio by Essay") + theme(legend.position = "bottom") 

ggplot() + geom_point(data = Madison.essay.sentiment.score,aes(x=Positive_Ratio,y=Negative_Ratio,size=Total_Score)) +labs (x="Positive Sentiment Ratio",y="Negative Sentiment Ratio",title = "Positive Vs Negative Sentiment Ratio for Madison Essays") + theme(legend.position = "bottom") 

```


## Load Dispute Essays
```{r Load Dispute Essays}

dispt.files <- list.files("C:/Users/rkrishnan/Documents/01 Personal/MS/IST 707/week5/temp/txt/", pattern="dispt", all.files=FALSE,full.names=FALSE)

 for(i in 1:length(dispt.files)){
  
sbaFile <- paste("C:/Users/rkrishnan/Documents/01 Personal/MS/IST 707/week5/temp/txt/",dispt.files[i],sep = "")

sba <- readLines(sbaFile)
str(sba)

# Text Transformation
words.vec <-VectorSource(sba)
words.corpus <-Corpus(words.vec)
words.corpus

words.corpus <- tm_map(words.corpus,content_transformer(tolower))
words.corpus <- tm_map(words.corpus,removePunctuation)
words.corpus <- tm_map(words.corpus,removeNumbers)
words.corpus <- tm_map(words.corpus,removeWords,stopwords("english"))

## Create Term Document Matrix

tdm <-TermDocumentMatrix(words.corpus)
(tdm)
m <-as.matrix(tdm)
wordCounts <- rowSums(m)
wordCounts <- sort(wordCounts,decreasing = TRUE)
head(wordCounts)
cloudFrame <-data.frame(word=names(wordCounts),freq=wordCounts)
#wordcloud(cloudFrame$word,cloudFrame$freq)

## Plot Word Cloud
# wordcloud(names(wordCounts),wordCounts,min.freq = 2,max.words = 50,rot.per = 0.35,colors = brewer.pal(8,"Dark2"))

## Sentiment Analysis


#calculate the total number of words
totalwords <- sum(wordCounts)

#have a vector that just has all the words
words <-names(wordCounts)
matched <- match(words,p,nomatch=0)

## For Testing positive words

matched.df <-data.frame(matched.id=as.character(seq_len(length(matched))),matched=matched)
matched.df <- matched.df[which(matched.df$matched!=0),]
paste(p[as.numeric(as.character(matched.df[1,2]))],"=",words[as.numeric(as.character(matched.df[1,1]))])
  
# Store Positive Counts
mCounts <-wordCounts[which(matched !=0)]
length(mCounts)

mWords <- names(mCounts)
nPos <- sum(mCounts)
nPos


matched <- match(words,n,nomatch=0)

## For Testing negative words
matched.df <-data.frame(matched.id=as.character(seq_len(length(matched))),matched=matched)
matched.df <- matched.df[which(matched.df$matched!=0),]
paste(n[as.numeric(as.character(matched.df[1,2]))],"=",words[as.numeric(as.character(matched.df[1,1]))])

# Store Negative Counts
nCounts <-wordCounts[which(matched !=0)]
length(nCounts)

nWords <- names(nCounts)
nNeg <- sum(nCounts)
nNeg

## Calculate the Sentiment

# calculate the % of words that are positive and negative
totalWords <-length(words)

ratioPos <-nPos/totalWords
ratioPos

ratioNeg <-nNeg/totalWords
ratioNeg


# compute the overall score using AFFIN word list

matched <-match(words,affin$word,nomatch=0)

## For Testing affinity match
matched.df <-data.frame(matched.id=as.character(seq_len(length(matched))),matched=matched)
matched.df <- matched.df[which(matched.df$matched!=0),]
paste(affin$word[as.numeric(as.character(matched.df[1,2]))],"=",words[as.numeric(as.character(matched.df[1,1]))], ", Score: ",affin$score[as.numeric(as.character(matched.df[1,2]))])


wordCounts[which(matched !=0)]
affin$word[matched[which(matched !=0)]]
affin$score[matched[which(matched !=0)]]
mScore <- affin$score[matched[which(matched !=0)]]

pScore <- sum(ifelse(mScore >0, mScore, 0))
nScore <- abs(sum(ifelse(mScore <0, mScore, 0)))
totalScore <- sum(abs(mScore))

# Overall Score 
totalScore
pScore
nScore


#ratio of  postive and negative  Score

ratioPosScore <-pScore/totalScore
ratioNegScore <-nScore/totalScore

ratioPosScore
ratioNegScore

fnGetSentimentScore(words,i)
file_nm[i] <-dispt.files[i]
}

## r Plot Sentiment Score

dispt.essay.sentiment.score <- data.frame(cbind(c(1:length(dispt.files)),file_nm[1:length(dispt.files)],totalScore.m[1:length(dispt.files)],pScore.m[1:length(dispt.files)],nScore.m[1:length(dispt.files)],ratioPosScore.m[1:length(dispt.files)],ratioNegScore.m[1:length(dispt.files)]))
dispt.essay.sentiment.score <-`colnames<-`(dispt.essay.sentiment.score,c("Essay","File_nm","Total_Score","Positive_Score","Negative_Score","Positive_Ratio","Negative_Ratio"))

# convert score to numeric data
dispt.essay.sentiment.score$Total_Score <- as.numeric(as.character(dispt.essay.sentiment.score$Total_Score))
dispt.essay.sentiment.score$Positive_Score <- as.numeric(as.character(dispt.essay.sentiment.score$Positive_Score))
dispt.essay.sentiment.score$Negative_Score <- as.numeric(as.character(dispt.essay.sentiment.score$Negative_Score))


options(digits = 4)
dispt.essay.sentiment.score$Positive_Ratio <- as.numeric(as.character(dispt.essay.sentiment.score$Positive_Ratio))
dispt.essay.sentiment.score$Negative_Ratio <- as.numeric(as.character(dispt.essay.sentiment.score$Negative_Ratio))

ggplot() + geom_bar(data = dispt.essay.sentiment.score,aes(x=Essay,y=Total_Score),stat="identity")+labs (x="Essay Number",y="Total Sentiment Score",title = "Total Sentiment Score by  Essay") + theme(legend.position = "bottom") 

ggplot() + geom_bar(data = dispt.essay.sentiment.score,aes(x=Essay,y=Positive_Score),stat="identity")+labs (x="Essay Number",y="Positive Sentiment Score",title = "Positive Sentiment Score by Essay") + theme(legend.position = "bottom") 

ggplot() + geom_bar(data = dispt.essay.sentiment.score,aes(x=Essay,y=Negative_Score),stat="identity")+labs (x="Essay Number",y="Negative Sentiment Score",title = "Negative Sentiment Score by Essay") + theme(legend.position = "bottom") 

ggplot() + geom_bar(data = dispt.essay.sentiment.score,aes(x=Essay,y=Positive_Ratio),stat="identity")+labs (x="Essay Number",y="Positive Sentiment Ratio",title = "Positive Sentiment Ratio by Essay") + theme(legend.position = "bottom") 

ggplot() + geom_bar(data = dispt.essay.sentiment.score,aes(x=Essay,y=Negative_Ratio),stat="identity")+labs (x="Essay Number",y="Negative Sentiment Ratio",title = "Negative Sentiment Ratio by Essay") + theme(legend.position = "bottom") 

ggplot() + geom_point(data = dispt.essay.sentiment.score,aes(x=Positive_Ratio,y=Negative_Ratio,size=Total_Score)) +labs (x="Positive Sentiment Ratio",y="Negative Sentiment Ratio",title = "Positive Vs Negative Sentiment Ratio for dispt Essays") + theme(legend.position = "bottom") 


```


```{r}
# combine H & M scores

Train.essay.sentiment.score <- rbind(Hamilton.essay.sentiment.score,Madison.essay.sentiment.score)
Train.essay.sentiment.label <- data.frame(auth.h.m=as.factor(Train.essay.sentiment.score$auth.h.m))
Train.essay.sentiment.score <- Train.essay.sentiment.score[,-c(1:2)]
Test.essay.sentiment.score <- dispt.essay.sentiment.score[,-c(1:2)]

# rescale(c(1:10000), to = c(0, 1), from = range(c(1:10000), na.rm = TRUE, finite = TRUE))
#Scale it down to common metrics
Train.essay.sentiment.score.scaled <- Train.essay.sentiment.score

Train.essay.sentiment.score.scaled$Total_Score <- scales::rescale(Train.essay.sentiment.score$Total_Score
                                                          ,to=c(1,100),from = range(Train.essay.sentiment.score$Total_Score, na.rm = TRUE, finite = TRUE)
                                                          )

Train.essay.sentiment.score.scaled$Positive_Score <- scales::rescale(Train.essay.sentiment.score$Positive_Score
                                                             ,to=c(1,100),from = range(Train.essay.sentiment.score$Positive_Score, na.rm = TRUE, finite = TRUE)
                                                             )
Train.essay.sentiment.score.scaled$Negative_Score <- scales::rescale(Train.essay.sentiment.score$Negative_Score
                                                             ,to=c(1,100),from = range(Train.essay.sentiment.score$Negative_Score, na.rm = TRUE, finite = TRUE)
                                                             )
Train.essay.sentiment.score.scaled$Positive_Ratio <- scales::rescale(Train.essay.sentiment.score$Positive_Ratio
                                                             ,to=c(1,100),from = range(Train.essay.sentiment.score$Positive_Ratio, na.rm = TRUE, finite = TRUE)
                                                             )
Train.essay.sentiment.score.scaled$Negative_Ratio <- scales::rescale(Train.essay.sentiment.score$Negative_Ratio
                                                             ,to=c(1,100),from = range(Train.essay.sentiment.score$Negative_Ratio, na.rm = TRUE, finite = TRUE)
                                                             )

Train.essay.sentiment.score.scaled$auth.h.m <-Train.essay.sentiment.label$auth.h.m

Test.essay.sentiment.score.scaled <- Test.essay.sentiment.score
Test.essay.sentiment.score.scaled$Total_Score <- scales::rescale(Test.essay.sentiment.score$Total_Score
                                                         ,to=c(1,100),from = range(Test.essay.sentiment.score$Total_Score, na.rm = TRUE, finite = TRUE)
                                                         )
Test.essay.sentiment.score.scaled$Positive_Score <- scales::rescale(Test.essay.sentiment.score$Positive_Score
                                                            ,to=c(1,100),from = range(Test.essay.sentiment.score$Positive_Score, na.rm = TRUE, finite = TRUE)
                                                            )
Test.essay.sentiment.score.scaled$Negative_Score <- scales::rescale(Test.essay.sentiment.score$Negative_Score
                                                            ,to=c(1,100),from = range(Test.essay.sentiment.score$Negative_Score, na.rm = TRUE, finite = TRUE)
                                                            )
Test.essay.sentiment.score.scaled$Positive_Ratio <- scales::rescale(Test.essay.sentiment.score$Positive_Ratio
                                                            ,to=c(1,100),from = range(Test.essay.sentiment.score$Positive_Ratio, na.rm = TRUE, finite = TRUE)
                                                            )
Test.essay.sentiment.score.scaled$Negative_Ratio <- scales::rescale(Test.essay.sentiment.score$Negative_Ratio
                                                            ,to=c(1,100),from = range(Test.essay.sentiment.score$Negative_Ratio, na.rm = TRUE, finite = TRUE)
                                                            )

`colnames<-`(Test.essay.sentiment.score.scaled,c("Total_Score","Positive_Score","Negative_Score","Positive_Ratio","Negative_Ratio"))

(Train.essay.sentiment.score.scaled)
(Test.essay.sentiment.score.scaled)
All.essay.sentiment.score.scaled <- rbind(Train.essay.sentiment.score.scaled,cbind(Test.essay.sentiment.score.scaled,auth.h.m=2))


ggplot() + geom_point(data = Train.essay.sentiment.score.scaled,aes(x=Positive_Ratio,y=Negative_Ratio,size=Total_Score,color=auth.h.m)) +labs (x="Positive Sentiment Ratio",y="Negative Sentiment Ratio",title = "Positive Vs Negative Sentiment Ratio for H&M Essays") + theme(legend.position = "bottom") 

ggplot() + geom_point(data = All.essay.sentiment.score.scaled,aes(x=Positive_Ratio,y=Negative_Ratio,size=Total_Score,color=auth.h.m)) +labs (x="Positive Sentiment Ratio",y="Negative Sentiment Ratio",title = "Positive Vs Negative Sentiment Ratio for H&M Essays") + theme(legend.position = "bottom")

# pairs.panels(All.essay.sentiment.score.scaled)

All.essay.sentiment.score.scaled %>%
  ggplot(aes(x=Positive_Ratio, fill=auth.h.m)) +
  geom_density(alpha=.7, color="black") +
  ggtitle("Essay Density Positive_Ratio")

All.essay.sentiment.score.scaled %>%
  ggplot(aes(x=Negative_Ratio, fill=auth.h.m)) +
  geom_density(alpha=.7, color="black") +
  ggtitle("Essay Density Positive_Ratio")

```

# create model based on sentiment score
```{r}

fit3.rpart <- rpart(auth.h.m ~., data = Train.essay.sentiment.score.scaled, method = "class" ,control = rpart.control(minsplit = 1, cp = 0.2))
summary(fit3.rpart)
predicted3.rpart=predict(fit3.rpart,Test.essay.sentiment.score.scaled, type="class")
(head(predicted3.rpart,n=11))
#(head(test, n=10))
#plot(fit3.rpart)
# text(fit3.rpart)
# fancyRpartPlot(fit3.rpart)

fit4.j48=J48(auth.h.m~., data = Train.essay.sentiment.score.scaled, control=Weka_control(U=FALSE, M=2, C=0.1))
e <- evaluate_Weka_classifier(fit4.j48, numFolds = 10, seed = 1, class = TRUE)
predicted2.j48=predict (fit4.j48, newdata = Test.essay.sentiment.score.scaled, type = c("class"))
InfoGainAttributeEval(auth.h.m ~ . , data = Train.essay.sentiment.score.scaled)
(fit4.j48)


```
