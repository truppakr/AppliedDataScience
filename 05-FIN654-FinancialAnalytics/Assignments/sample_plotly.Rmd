---
title: "plotly_test"
author: "ThulasiRamRuppaKrishnan"
date: "September 2, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
# setwd("C:\\Users\\rkrishnan\\Documents\\01 Personal\\MS\\FIN654\\Project")
# coffee_currency <- readxl::read_xlsx("RamData (1).xlsx",sheet = "Coffee&Currency",col_names = TRUE)


options(digits = 5)
setwd("C:\\Users\\rkrishnan\\Documents\\01 Personal\\MS\\FIN654\\Project")
coffee_currency <- read.csv("data/coffee&currency.csv", header = T, stringsAsFactors = F)
coffee_currency <- na.omit(coffee_currency[,1:7])

# Construct expanded data frame
return <- as.numeric(diff(log(coffee_currency$KC1))) * 100 # Euler 
size <- as.numeric(abs(return)) # size is indicator of volatility
direction <- ifelse(return > 0, "up", ifelse(return < 0, "down", "same")) # another indicator of volatility
# =if(return > 0, "up", if(return < 0, "down", "same"))
date <- as.Date(coffee_currency$Date[-1], "%m/%d/%Y") # length of DATE is length of return +1: omit 1st observation
price <- as.numeric(coffee_currency$KC1[-1]) # length of DHOILNYH is length of return +1: omit first observation
coffee.df <- na.omit(data.frame(date = date, price = price, return = return, size = size, direction = direction)) # clean up data frame by omitting NAs
str(coffee.df)



```

```{r}
library(ggplot2)
p <- ggplot(coffee.df, aes(x = date, y = return, group = 1)) + geom_line(colour = "blue")
p
```
```{r}
# library(ggplot2)
p <- ggplot(coffee.df, aes(x = date, y = size, group = 1)) + geom_bar(stat = "identity", colour = "green")
p
```
```{r}
p <- ggplot(coffee.df, aes(date, size)) + geom_bar(stat = "identity", colour = "darkorange") + geom_line(data = coffee.df, aes(date, return), colour = "blue")
p
```
```{r}
# Load the data_moments() function
## data_moments function
## INPUTS: vector
## OUTPUTS: list of scalars (mean, sd, median, skewness, kurtosis)
data_moments <- function(data){
  library(moments)
  mean.r <- mean(data)
  sd.r <- sd(data)
  median.r <- median(data)
  skewness.r <- skewness(data)
  kurtosis.r <- kurtosis(data)
  result <- data.frame(mean = mean.r, std_dev = sd.r, median = median.r, skewness = skewness.r, kurtosis = kurtosis.r)
  return(result)
}
# Run data_moments()
answer <- data_moments(coffee.df$return)
# Build pretty table
answer <- round(answer, 4)
knitr::kable(answer)
```

```{r}
# Counting
table(coffee.df$return < 0) # one way
table(coffee.df$return > 0)
table(coffee.df$direction) # this counts 0 returns as negative
table(coffee.df$return == 0)
# Pivoting
library(dplyr)
## 1: filter to those houses with fairly high prices
# pivot.table <-  filter(HO2.df, size > 0.5*max(size))
## 2: set up data frame for by-group processing
pivot.table <-  group_by(coffee.df, direction)
## 3: calculate the summary metrics
options(dplyr.width = Inf) ## to display all columns
coffee.count <- length(coffee.df$return)
pivot.table <-  summarise(pivot.table, return.avg = round(mean(return), 4), return.sd = round(sd(return), 4), quantile.5 = round(quantile(return, 0.05), 4), quantile.95 = round(quantile(return, 0.95), 4), percent = round((length(return)/coffee.count)*100, 2))
# Build visual
knitr::kable(pivot.table, digits = 2)
# # Here is how we can produce a LaTeX formatted and rendered table
# # 
# library(xtable)
# HO2.caption <- "Heating Oil No. 2: 1986-2016"
# print(xtable(t(pivot.table), digits = 2, caption = HO2.caption, align=rep("r", 4), table.placement="V"))
# print(xtable(answer), digits = 2)
```

```{r}

coffee.tol.pct <- 0.95
coffee.tol <- quantile(coffee.df$return, coffee.tol.pct)
coffee.tol.label <- paste("Tolerable Rate = ", round(coffee.tol, 2), sep = "")
ggplot(coffee.df, aes(return, fill = direction)) + stat_ecdf(colour = "blue", size = 0.75) + geom_vline(xintercept = coffee.tol, colour = "red", size = 1.5) + annotate("text", x = coffee.tol+1 , y = 0.75, label = coffee.tol.label, colour = "darkred")

```
```{r}
## HO2_movement(file, caption)
## input: HO2 csv file from /data directory
## output: result for input to kable in $table and xtable in $xtable; 
##         data frame for plotting and further analysis in $df.
## Example: HO2.data <- HO2_movement(file = "data/nyhh02.csv", caption = "HO2 NYH")
coffee_movement <- function(file ="coffee&currency.csv", caption = "Coffee"){
  options(digits = 5)
  setwd("C:\\Users\\rkrishnan\\Documents\\01 Personal\\MS\\FIN654\\Project")
  # Read file and deposit into variable
  coffee <- read.csv(file, header = T, stringsAsFactors = F)
  coffee <- na.omit(coffee[,1:7])
  # stringsAsFactors sets dates as character type
  coffee <- na.omit(coffee) ## to clean up any missing data
  # Construct expanded data frame
  return <- as.numeric(diff(log(coffee$KC1))) * 100
  size <- as.numeric(abs(return)) # size is indicator of volatility
  direction <- ifelse(return > 0, "up", ifelse(return < 0, "down", "same")) # another indicator of volatility
  date <- as.Date(coffee$Date[-1], "%m/%d/%Y") # length of DATE is length of return +1: omit 1st observation
  price <- as.numeric(coffee$KC1[-1]) # length of DHOILNYH is length of return +1: omit first observation
  coffee.df <- na.omit(data.frame(date = date, price = price, return = return, size = size, direction = direction)) # clean up data frame by omitting NAs
  require(dplyr)
  ## 1: filter if necessary
  # pivot.table <-  filter(HO2.df, size > 0.5*max(size))
  ## 2: set up data frame for by-group processing
  pivot.table <-  group_by(coffee.df, direction)
  ## 3: calculate the summary metrics
  options(dplyr.width = Inf) ## to display all columns
  coffee.count <- length(coffee.df$return)
  pivot.table <-  summarise(pivot.table, return.avg = mean(return), return.sd = sd(return), quantile.5 = quantile(return, 0.05), quantile.95 = quantile(return, 0.95), percent = (length(return)/coffee.count)*100)
  # Construct transpose of pivot table with xtable()
  # require(xtable)
  # pivot.xtable <- xtable(t(pivot.table), digits = 2, caption = caption, align=rep("r", 4), table.placement="V")
  # HO2.caption <- "Heating Oil No. 2: 1986-2016"
  # output.list <- list(table = pivot.table, xtable = pivot.xtable, df = HO2.df)
  output.list <- list(table = pivot.table, df = coffee.df)
return(output.list)
}
```

```{r}
knitr::kable(coffee_movement(file = "data/coffee&currency.csv")$table, digits = 5)
```

```{r}
library(MASS)
coffee.data <- coffee_movement(file = "data/coffee&currency.csv", caption = "Coffee")$df
str(coffee.data)
fit.gamma.up <- fitdistr(coffee.data[coffee.data$direction == "up", "return"], "gamma", hessian = TRUE)
fit.gamma.up
# fit.t.same <- fitdistr(HO2.data[HO2.data$direction == "same", "return"], "gamma", hessian = TRUE) # a problem here is all observations = 0
fit.t.down <- fitdistr(coffee.data[coffee.data$direction == "down", "return"], "t", hessian = TRUE)
fit.t.down
fit.gamma.down <- fitdistr(-coffee.data[coffee.data$direction == "down", "return"], "gamma", hessian = TRUE) # gamma distribution defined for data >= 0
fit.gamma.down
```


```{r}
# Read in data
library(zoo)
library(xts)
library(ggplot2)
# Read and review a csv file from FRED
 exrates <- coffee_currency[,c(2,4,5,6)]
# exrates <- na.omit(read.csv("data/exrates.csv", header = TRUE))
# Check the data
 exrates$Date <- as.factor(exrates$Date)
head(exrates)
tail(exrates)
str(exrates)
# Begin to explore the data
summary(exrates)
```


```{r}
# Compute log differences percent using as.matrix to force numeric type
exrates.r <- diff(log(as.matrix(exrates[, -1]))) * 100
head(exrates.r)
tail(exrates.r)
str(exrates.r)
# Create size and direction
size <- na.omit(abs(exrates.r)) # size is indicator of volatility
head(size)
# colnames(size) <- paste(colnames(size),".size", sep = "") # Teetor
direction <- ifelse(exrates.r > 0, 1, ifelse(exrates.r < 0, -1, 0)) # another indicator of volatility
# colnames(direction) <- paste(colnames(direction),".dir", sep = "")
head(direction)
# Convert into a time series object: 
# 1. Split into date and rates
dates <- as.Date(exrates$Date[-1], "%m/%d/%Y")
values <- cbind(exrates.r, size, direction)
# for dplyr pivoting we need a data frame
exrates.df <- data.frame(dates = dates, returns = exrates.r, size = size, direction = direction)
str(exrates.df) # notice the returns.* and direction.* prefixes
# 2. Make an xts object with row names equal to the dates
exrates.xts <- na.omit(as.xts(values, dates)) #order.by=as.Date(dates, "%d/%m/%Y")))
str(exrates.xts)
exrates.zr <- na.omit(as.zooreg(exrates.xts))
str(exrates.zr)
head(exrates.xts)
```

```{r}
library(ggplot2)
library(plotly)
title.chg <- "Exchange Rate Percent Changes"
p1 <- autoplot.zoo(exrates.xts[,1:3]) + ggtitle(title.chg) + ylim(-5, 5)
p2 <- autoplot.zoo(exrates.xts[,4:6]) + ggtitle(title.chg) + ylim(-5, 5)
ggplotly(p1)
```
```{r}
acf(coredata(exrates.xts[ , 1:3])) # returns
acf(coredata(exrates.xts[ , 4:6])) # sizes
pacf(coredata(exrates.xts[ , 1:3])) # returns
pacf(coredata(exrates.xts[ , 4:6])) # sizes
# Load the data_moments() function
## data_moments function
## INPUTS: r vector
## OUTPUTS: list of scalars (mean, sd, median, skewness, kurtosis)
data_moments <- function(data){
  library(moments)
  library(matrixStats)
  mean.r <- colMeans(data)
  median.r <- colMedians(data)
  sd.r <- colSds(data)
  IQR.r <- colIQRs(data)
  skewness.r <- skewness(data)
  kurtosis.r <- kurtosis(data)
  result <- data.frame(mean = mean.r, median = median.r, std_dev = sd.r, IQR = IQR.r, skewness = skewness.r, kurtosis = kurtosis.r)
  return(result)
}
# Run data_moments()
answer <- data_moments(exrates.xts[, 4:6])
# Build pretty table
answer <- round(answer, 3)
knitr::kable(answer)
mean(exrates.xts[,3])
```

```{r }
exrates.tol.pct <- 0.95
exrates.tol <- quantile(exrates.df$returns.EUR, exrates.tol.pct)
exrates.tol.label <- paste("Tolerable Rate = ", round(exrates.tol, 2), "%", sep = "")
p <- ggplot(exrates.df, aes(returns.EUR, fill = direction.EUR)) + stat_ecdf(colour = "blue", size = 0.75, geom = "point") + geom_vline(xintercept = exrates.tol, colour = "red", size = 1.5) + annotate("text", x = exrates.tol + 0.5 , y = 0.75, label = exrates.tol.label, colour = "darkred")
ggplotly(p)
```

```{r}
one <- ts(exrates.df$returns.EUR)
two <- ts(exrates.df$returns.CHF)
# or
one <- ts(exrates.zr[,1])
two <- ts(exrates.zr[,2])
ccf(abs(one), abs(two), main = "CHF vs. EUR", lag.max = 20, xlab = "", ylab = "", ci.col = "red")
# build function to repeat these routines
run_ccf <- function(one, two, main = "one vs. two", lag = 20, color = "red"){
  # one and two are equal length series
  # main is title
  # lag is number of lags in cross-correlation
  # color is color of dashed confidence interval bounds
  stopifnot(length(one) == length(two))
  one <- ts(one)
  two <- ts(two)
  main <- main
  lag <- lag
  color <- color
  ccf(one, two, main = main, lag.max = lag, xlab = "", ylab = "", ci.col = color)
  #end run_ccf
}
one <- ts(exrates.df$returns.EUR)
two <- ts(exrates.df$returns.CHF)
# or
one <- exrates.zr[,1]
two <- exrates.zr[,2]
title <- "EUR vs. GBP"
run_ccf(abs(one), abs(two), main = title, lag = 20, color = "red")
# now for volatility (sizes)
one <- ts(abs(exrates.zr[,1]))
two <- ts(abs(exrates.zr[,2]))
title <- "EUR vs. CHF: volatility"
run_ccf(one, two, main = title, lag = 20, color = "red")
# We see some small raw correlations across time with raw returns. More revealing, we see volatility of correlation clustering using return sizes. 
```
```{r}
corr_rolling <- function(x) {	
  dim <- ncol(x)	
  corr_r <- cor(x)[lower.tri(diag(dim), diag = FALSE)]	
  return(corr_r)	
}
vol_rolling <- function(x){
  library(matrixStats)
  vol_r <- colSds(x)
  return(vol_r)
}
ALL.r <- exrates.xts[, 1:3]
window <- 90 #reactive({input$window})
corr_r <- rollapply(ALL.r, width = window, corr_rolling, align = "right", by.column = FALSE)
colnames(corr_r) <- c("EUR.CAD", "EUR.CHF", "CAD.CHF")
vol_r <- rollapply(ALL.r, width = window, vol_rolling, align = "right", by.column = FALSE)
colnames(vol_r) <- c("EUR.vol", "CAD.vol", "CHF.vol")
year <- format(index(corr_r), "%Y")
r_corr_vol <- merge(ALL.r, corr_r, vol_r, year)
```
```{r}
library(quantreg)
taus <- seq(.05,.95, .05)	# Roger Koenker UIC Bob Hogg and Allen Craig
fit.rq.CAD.CHF <- rq(log(CAD.CHF) ~ log(CHF.vol), tau = taus, data = r_corr_vol)	
fit.lm.CAD.CHF <- lm(log(CAD.CHF) ~ log(CHF.vol), data = r_corr_vol)	
# Some test statements	
CAD.CHF.summary <- summary(fit.rq.CAD.CHF, se = "boot")
CAD.CHF.summary
plot(CAD.CHF.summary)
#

```

```{r}
library(quantreg)
library(magick)
img <- image_graph(res = 96)
datalist <- split(r_corr_vol, r_corr_vol$year)
out <- lapply(datalist, function(data){
  p <- ggplot(data, aes(CHF.vol, CAD.CHF)) +
    geom_point() + 
    ggtitle(data$year) + 
    geom_quantile(quantiles = c(0.05, 0.95)) + 
    geom_quantile(quantiles = 0.5, linetype = "longdash") +
    geom_density_2d(colour = "red")  
  print(p)
})
while (!is.null(dev.list()))  dev.off()
#img <- image_background(image_trim(img), 'white')
animation <- image_animate(img, fps = .5)
animation	
```

```{r}
library(dplyr)
x <- as.numeric(exrates.df$returns.CAD) # USD.EUR
y <- as.numeric(exrates.df$returns.CHF) # USD.GBP
xy.df <- na.omit(data.frame(date = dates, ahead_x= lead(x, 5), behind_y = lag(y, 5)))
yx.df <- na.omit(data.frame(date = dates, ahead_y =lead(y, 5), behind_x = lag(x, 5)))
answer <- data_moments(na.omit(as.matrix(xy.df[,2:3])))
answer <- round(answer, 4)
knitr::kable(answer)
answer <- data_moments(na.omit(as.matrix(yx.df[,2:3])))
answer <- round(answer, 4)
knitr::kable(answer)
cor(as.numeric(xy.df$ahead_x), as.numeric(xy.df$behind_y))
cor(as.numeric(yx.df$ahead_y), as.numeric(yx.df$behind_x))
```


```{r}
options(digits = 4, scipen = 999999)
library(flexdashboard)
library(shiny)
library(psych)
library(ggplot2)
library(GGally)
library(lubridate)
library(dplyr)
library(quantreg)
library(forecast)
library(tidyquant)
library(timetk)
library(quantmod)
library(matrixStats)
library(QRM)
library(quadprog)
#
metals_env <- new.env()
symbols <- c("JJC", "JJN", "JJU")
getSymbols(symbols) #, env = stocks_env) # using quantmod
data <- JJC # COPPER
data <- data[ , 6] # only adjusted close  
colnames(data) <- "copper"
r_JJC <- diff(log(data))[-1] 
# convert xts object to a tibble or data frame
p_JJC <- data %>% as_tibble() %>% mutate(date = index(data), month = month.abb[month(index(data))])
# repeat
data <- JJN #NICKEL
data <- data[ , 6]  
colnames(data) <- "nickel"
r_JJN <- diff(log(data))[-1]
p_JJN <- data %>% as_tibble() %>% mutate(date = index(data), month = month.abb[month(index(data))])
# and again
data <- JJU
data <- data[ , 6]  
colnames(data) <- "aluminium"
r_JJU <- diff(log(data))[-1]
p_JJU <- data %>% as_tibble() %>% mutate(date = index(data), month = month.abb[month(index(data))])#rate_IYM <- data %>% mutate(diff(log(p_IYM))[-1])
# merge by date (as row name)
price <- exrates.xts[,1:3] %>% as_tibble() %>% mutate(date = index(exrates.xts[,1:3]), month = month.abb[month(index(exrates.xts[,1:3]))])
return <- exrates.xts[,1:3]
# calculute within month correlations and choose lower triangle of correlation matrix
r_corr <- apply.monthly(return, FUN = cor)[, c(2, 3, 6)]
colnames(r_corr) <- c("EUR", "CAD", "CHF")
# calculate within month standard deviations using MatrixStats
r_vols <- apply.monthly(return, FUN = colSds)
# long format ("TIDY") price tibble for possible other work
price_tbl <- price %>% as_tibble() %>% gather(k = symbol, value = price, EUR, CAD, CHF )  %>% dplyr::select(symbol, date, price)
return_tbl <- price_tbl %>% group_by(symbol) %>% tq_transmute(mutate_fun = periodReturn, period = "daily", type = "log", col_rename = "daily_return") %>% mutate(abs_return = abs(daily_return))
str(return_tbl)
# 
corr_tbl <- r_corr %>% as_tibble() %>% mutate(date = index(r_corr)) %>% gather(key = assets, value = corr, -date)
vols_tbl <- r_vols %>% as_tibble() %>% mutate(date = index(r_vols)) %>% gather(key = assets, value = vols, -date) 
#
corr_vols <- merge(r_corr, r_vols)
corr_vols_tbl <- corr_vols %>% as_tibble() %>% mutate(date = index(corr_vols))
#
n <-  10000 # lots of trials, each a "day" or an "hour"
z <- rt(n, df = 30)
garch_sim_t <- function(n = 1000, df = 30, omega = 0.1, alpha = 0.8, phi = 0.05, mu = 0.01){
  n <- n # lots of trials, each a "day" or an "hour"
  # set.seed(seed)
  z <- rt(n, df = df) 
  e <-  z # store variates
  y <-  z # returns: store again in a different place
  sig2 <-  z^2 # create volatility series
  omega <-  omega #base variance
  alpha <-  alpha #vols Markov dependence on previous variance
  phi <-  phi # returns Markov dependence on previous period
  mu <-  mu # average return
  for (t in 2:n) { # Because of lag start at second
    e[t] <- sqrt(sig2[t])*z[t]           # 1. e is conditional on sig
    y[t] <-  mu + phi*(y[t-1]-mu) + e[t] # 2. generate returns
    sig2[t+1] <-  omega + alpha * e[t]^2 # 3. generate new sigma^2
    }
  return <- list(
    sim_df_vbl <- data_frame(t = 1:n, z = z, y = y, e = e, sig = sqrt(sig2)[-(n+1)] ),
    sim_df_title <- data_frame(t = 1:n, "1. Unconditional innovations" = z, "4. Conditional returns" = y, "3. Conditional innovations" = e, "2. Conditional volatility" = sqrt(sig2)[-(n+1)] )
  )
}
#
price_etf <- price  %>% dplyr::select(EUR, CAD, CHF) # 3 risk factors (rf)
price_0 <- as.numeric(tail(price_etf, 1))
shares <- c(60000, 75000, 50000)
price_last <- price[length(price$EUR), 1:3] #(TAN, ICLN, PBW) %>% as.vector()
w <- as.numeric(shares * price_last)
return_hist <- na.omit(apply((price[, 1:3]), 2, diff))
# Fan these across the length and breadth of the risk factor series
weights_rf <- matrix(w, nrow=nrow(return_hist), ncol=ncol(return_hist), byrow=TRUE)
## We need to compute exp(x) - 1 for very small x: expm1 accomplishes this
loss_rf <- -rowSums(expm1(return_hist) * weights_rf)
loss_df <- data_frame(loss = loss_rf, distribution = rep("historical", each = length(loss_rf)))
#
ES_calc <- function(data, prob){
  threshold <- quantile(data, prob)
  result <- mean(data[data > threshold])
}

#
n_sim <- 1000
n_sample <- 100
prob <- 0.95
ES_sim <- replicate(n_sim, ES_calc(sample(loss_rf, n_sample, replace = TRUE), prob))
#
summary(ES_sim)
#
# mean excess plot to determine thresholds for extreme event management
data <- as.vector(loss_rf) # data is purely numeric
umin <-  min(data)         # threshold u min
umax <-  max(data) - 0.1   # threshold u max
nint <- 100                # grid length to generate mean excess plot
grid_0 <- numeric(nint)    # grid store
e <- grid_0                # store mean exceedances e
upper <- grid_0            # store upper confidence interval
lower <- grid_0            # store lower confidence interval
u <- seq(umin, umax, length = nint) # threshold u grid
alpha <- 0.95                  # confidence level
for (i in 1:nint) {
    data <- data[data > u[i]]  # subset data above thresholds
    e[i] <- mean(data - u[i])  # calculate mean excess of threshold
    sdev <- sqrt(var(data))    # standard deviation
    n <- length(data)          # sample size of subsetted data above thresholds
    upper[i] <- e[i] + (qnorm((1 + alpha)/2) * sdev)/sqrt(n) # upper confidence interval
    lower[i] <- e[i] - (qnorm((1 + alpha)/2) * sdev)/sqrt(n) # lower confidence interval
  }
mep_df <- data.frame(threshold = u, threshold_exceedances = e, lower = lower, upper = upper)
loss_excess <- loss_rf[loss_rf > u] - u
quantInv <- function(distr, value) ecdf(distr)(value)
u_prob <- quantInv(loss_rf, 200000)
ES_mep <- mean(loss_rf[loss_rf > quantile(loss_rf, u_prob)])
##
#
data_moments <- function(data){
  library(moments)
  library(matrixStats)
  mean <- colMeans(data)
  median <- colMedians(data)
  sd <- colSds(data)
  IQR <- colIQRs(data)
  skewness <- skewness(data)
  kurtosis <- kurtosis(data)
  result <- data.frame(mean = mean, median = median, std_dev = sd, IQR = IQR, skewness = skewness, kurtosis = kurtosis)
  return(result)
}
#
##
# Portfolio Analytics
##
#
contract <- 1 # billion
working <- 0.100 # billion
sigma_wc <- 0.025 # billion
sigma <- 0.25
threshold <- -0.12 # percentage return
alpha <- 0.05 # tolerance
risky <- 0.1 # percentage return on the risky asset
riskless <- 0.02 # time value of cash -- no risk
z_star <- qnorm(alpha)
w <- (threshold-riskless) / (risky - riskless + sigma*z_star)
#
# 2 risky assets and a risk-free asset
# per annum returns from line 148 above
# watch out for na!
return <- na.omit(return)
port_stats <- data_moments(return)
port_stats
rho_all <- cor(return)
# choose quantile scenario
# uprob <- 0.50
mu_1 <- abs(port_stats[1, 1] * 252)  #EUR
mu_2 <- abs(port_stats[3, 1] * 252)  #CHF
sig_1 <- port_stats[1, 3] * sqrt(252)
sig_2 <- port_stats[3, 3] * sqrt(252)
rho <- rho_all[3, 1]
r_f <-  0.03
w <-  seq(0, 2, len = 500) # we might need to adjust 5 downward
means <-  mu_2 + (mu_1 - mu_2) * w
var <-  sig_1^2 * w^2 + sig_2^2 * (1 - w)^2 +2*w*(1-w)*rho*sig_1*sig_2
risk <-  sqrt(var)
# plotting
sigma_mu_df <- data_frame(sigma_P = risk, mu_P = means )
names_R <- c("JJC", "JJU")
mean_R <- c(mu_1, mu_2)
sd_R <- c(sig_1, sig_2)
mu_P <- sigma_mu_df$mu_P
sigma_P <- sigma_mu_df$sigma_P
r_free <-  r_f ## input value of risk-free interest rate
# sharpe ratio and minimum variance portfolio analysis
sharpe <- (mu_P - r_free)/sigma_P ## compute Sharpe's ratios
ind_max <-  (sharpe == max(sharpe)) ## Find maximum Sharpe's ratio
ind_min <-  (sigma_P == min(sigma_P)) ## find the minimum variance portfolio
ind_eff <-  (mu_P > mu_P[ind_min]) ## finally the efficient frontier
col_P <- ifelse(mu_P > mu_P[ind_min], "blue", "grey") # discriminate efficient and inefficient portfolios
sigma_mu_df$col_P <- col_P
# plot
p <- ggplot(sigma_mu_df, aes(x = sigma_P, y = mu_P, group = 1)) + geom_line(aes(colour=col_P, group = col_P)) + scale_colour_identity() # + xlim(0, max(sd_R*1.1))  + ylim(0, max(mean_R)*1.1) + 
p <- p + geom_point(aes(x = 0, y = r_free), colour = "red")
options(digits=4)
p <- p + geom_abline(intercept = r_free, slope = (mu_P[ind_max]-r_free)/sigma_P[ind_max], colour = "red")
p <- p + geom_point(aes(x = sigma_P[ind_max], y = mu_P[ind_max])) 
p <- p + geom_point(aes(x = sigma_P[ind_min], y = mu_P[ind_min])) ## show min var portfolio
p <- p + annotate("text", x = sd_R[1], y = mean_R[1], label = names_R[1]) + annotate("text", x = sd_R[2], y = mean_R[2], label = names_R[2]) 
p <- p + ylim(-.05, 0.15)
p
ggplotly(p) #if you like
#
# Many assets now
#
#R <-  (dat[2:n, -1]/dat[1:(n-1), -1] - 1) # or
#R <-  log(dat[2:n, -1]/dat[1:(n-1), -1])
R <-  na.omit(return) # daily returns from line 148?
n <- dim(R)[1]
N <- dim(R)[2]
R_boot <-  R[sample(1:n, 252),] # sample returns and lightning does not strike twice
r_free <- 0.03 / 252 # daily
mean_vect <-  apply(R_boot,2,mean)
cov_mat <-  cov(R_boot)
sd_vect <-  sqrt(diag(cov_mat))
A_mat <-  cbind(rep(1,N),mean_vect) 
mu_P <-  seq(-.01,.01,length=300)                              
sigma_P <-  mu_P 
weights <-  matrix(0,nrow=300,ncol=N) 
for (i in 1:length(mu_P))  
  {
    b_vec <-  c(1,mu_P[i])  
    result <-  
      solve.QP(Dmat=2*cov_mat,dvec=rep(0,N),Amat=A_mat,bvec=b_vec,meq=2)
    sigma_P[i] <-  sqrt(result$value)
    weights[i,] <-  result$solution
} 
# make a data frame of the mean and standard deviation results
sigma_mu_df <- data_frame(sigma_P = sigma_P, mu_P = mu_P)
names_R <- c("Cu", "Ni", "Al")
# sharpe ratio and minimum variance portfolio analysis
sharpe <- (mu_P - r_free)/sigma_P ## compute Sharpe's ratios
ind_max <-  (sharpe == max(sharpe)) ## Find maximum Sharpe's ratio
ind_min <-  (sigma_P == min(sigma_P)) ## find the minimum variance portfolio
ind_eff <-  (mu_P > mu_P[ind_min]) ## finally the efficient fr(aes(x = 0, y = r_free), colour = "red")ontier
col_P <- ifelse(mu_P > mu_P[ind_min], "blue", "grey") # discriminate efficient and inefficient portfolios
sigma_mu_df$col_P <- col_P
# plot it up
p <- ggplot(sigma_mu_df, aes(x = sigma_P, y = mu_P, group = 1))
p <- p + geom_line(aes(colour=col_P, group = col_P), size = 1.05) + scale_colour_identity() 
p <- p + geom_abline(intercept = r_free, slope = (mu_P[ind_max]-r_free)/sigma_P[ind_max], color = "red", size = 1.05)
p <- p + geom_point(aes(x = sigma_P[ind_max], y = mu_P[ind_max]), color = "green", size = 4) 
p <- p + geom_point(aes(x = sigma_P[ind_min], y = mu_P[ind_min]), color = "red", size = 4) ## show min var portfolio
p
ggplotly(p)
#
# helper function to support bootstrapping of tangency portfolio mean and sd
#
port_sample <- function(return, n_sample = 252, stat = "mean")
{
  R <-  return # daily returns
  n <- dim(R)[1]
  N <- dim(R)[2]
  R_boot <-  R[sample(1:n, n_sample, replace = TRUE),] # sample returns
  r_free <- 0.03 / 252 # daily
  mean_vect <-  apply(R_boot,2,mean)
  cov_mat <-  cov(R_boot)
  sd_vect <-  sqrt(diag(cov_mat))
  A_mat <-  cbind(rep(1,N),mean_vect) 
  mu_P <-  seq(-.01,.01,length=300)                              
  sigma_P <-  mu_P 
  weights <-  matrix(0,nrow=300,ncol=N) 
  for (i in 1:length(mu_P))  
  {
    b_vec <-  c(1,mu_P[i])  
    result <-  
      solve.QP(Dmat=2*cov_mat,dvec=rep(0,N),Amat=A_mat,bvec=b_vec,meq=2)
    sigma_P[i] <-  sqrt(result$value)
    weights[i,] <-  result$solution
  }
  sharpe <- (mu_P - r_free)/sigma_P ## compute Sharpe's ratios
  ind_max <-  (sharpe == max(sharpe)) ## Find maximum Sharpe's ratio
  ind_min <-  (sigma_P == min(sigma_P)) ## find the minimum variance portfolio
  ind_eff <-  (mu_P > mu_P[ind_min]) ## finally the efficient fr(aes(x = 0, y = r_free), colour = "red")ontier
  result <- switch(stat,
    "mean"  = mu_P[ind_max],
    "sd"    = sigma_P[ind_max]
    )
  return(result)
}
#
# try this sampling where 252 is a year of business days typically
#
port_mean <- replicate(1000, port_sample(return, n_sample = 252, stat = "mean"))
sim <- port_mean * 252
low <- quantile(sim, 0.025)
high <- quantile(sim, 0.975)
sim_df <- data_frame(sim = sim)
title <- "Tangency portfolio sampled mean simulation"
p <- ggplot(data = sim_df, aes(x = sim))
p <- p + geom_histogram(bins = 50, alpha = 0.7)
p <- p + ggtitle(title)
p <- p + geom_vline(xintercept = low, color = "red", size = 1.5 ) + geom_vline(xintercept = high, color = "red", size = 1.5)
p <- p + annotate("text", x = low + 0.1, y = 0.005, label = paste("L = ", round(low, 2))) + annotate("text", x = high, y = 0.005, label = paste("U = ", round(high, 2))) + ylab("density") + xlim(0, max(sim)+1) + xlab("daily mean: max Sharpe Ratio") + theme_bw()
p
#
options(digits = 2, scipen = 99999)
#
port_mean <- replicate(1000, port_sample(return, stat = "mean"))
port_sd <- replicate(1000, port_sample(return, stat = "sd"))
r_f <- 0.03
# choose one tangency portfolio scenario
mu <-  quantile((port_mean[port_mean*252 > r_f]*252), 0.05)
sigma <- quantile((port_sd*sqrt(252)), 0.05)
threshold <- -0.12
alpha <- 0.05
z_star <-  qnorm(alpha)
w_star <- (threshold-r_f) / (mu - r_f + sigma*z_star)
sigma_p <- seq(0, sigma * (1.1*w_star), length.out = 100)
mu_p <- r_f + (mu - r_f)*sigma_p/sigma
w <- sigma_p / sigma
sim_df <- data_frame(sigma_p = sigma_p, mu_p = mu_p, w = w)
#
label_42 <- paste(round(w_star*100, 2), "% risky asset", sep = "")
label_0 <- paste(alpha*100, "% alpha, ", threshold*100, "% threshold")
label_100 <- paste(1.00*100, "% risky asset \n mu = ", round(mu*100,2), "%\n sigma = ", round(sigma*100,2), "%", sep = "")
options(digits = 4)
p <- ggplot(sim_df, aes(x = sigma_p, y = mu_p)) + 
  geom_line(color = "blue", size = 1.1)
p <- p + geom_point(aes(x = 0.0 * sigma, y = r_f + (mu-r_f)*0.0), color = "red", size = 3.0) +
  geom_point(aes(x = w_star * sigma, y = r_f + (mu-r_f)*w_star), shape = 21, color = "red", fill = "white", size = 4, stroke = 4) + 
  annotate("text", x = w_star * sigma, y = r_f + (mu-r_f)*w_star + 0.01, label = label_42) +
  geom_point(aes(x = 1.0 * sigma, y = r_f + (mu-r_f)*1.00), color = "red", size = 3.0) + 
  annotate("text", x = 1.0 * sigma, y = r_f + (mu-r_f)*1.00 + 0.01, label = label_100) +
  xlab("standard deviation of portfolio return") +
  ylab("mean of portfolio return") +
  ggtitle(label_0)
ggplotly(p)
```


```{r}
rm(list = ls())
library(ggplot2)
library(flexdashboard)
library(shiny)
library(QRM)
library(qrmdata)
library(xts)
library(zoo)
library(plotly)
#library(ggfortify)
library(psych)
library(matrixStats)
library(moments)
library(quantreg)
library(quadprog)
library(scales)

options(digits = 5)
setwd("C:\\Users\\rkrishnan\\Documents\\01 Personal\\MS\\FIN654\\Project")
# wrco <- read.csv("WRCO index.csv", header = T, stringsAsFactors = F)
# wrco <- na.omit(wrco[,1:3])
# # stringsAsFactors sets dates as character type
# wrco <- na.omit(wrco) ## to clean up any missing data
# # Construct expanded data frame
# coffee <- read.csv("coffee&currency.csv", header = T, stringsAsFactors = F)
# coffee <- na.omit(coffee[,c(2,3,7)])

wrco <- read.csv("data/PTEAUSDM.csv", header = T, stringsAsFactors = F)
# wrco <- na.omit(wrco[,1:3])
# stringsAsFactors sets dates as character type
wrco <- na.omit(wrco) ## to clean up any missing data
# Construct expanded data frame
coffee <- read.csv("data/PCOFFROBUSDM.csv", header = T, stringsAsFactors = F)

# PAGE: Exploratory Analysis

prices <- merge(coffee,wrco,by.x="DATE",by.y="DATE")
# prices <- prices[,c(1,2,3,5)]
data <- prices
# Compute log differences percent using as.matrix to force numeric type
data.r <- diff(log(as.matrix(prices[, -1]))) * 100
# Create size and direction
size <- na.omit(abs(data.r)) # size is indicator of volatility
#head(size)
colnames(size) <- paste(colnames(size),".size", sep = "") # Teetor
direction <- ifelse(data.r > 0, 1, ifelse(data.r < 0, -1, 0)) # another indicator of volatility
colnames(direction) <- paste(colnames(direction),".dir", sep = "")
# Convert into a time series object: 
# 1. Split into date and rates
dates <- as.Date(data$DATE[-1], "%Y-%m-%d")
dates.chr <- as.character(data$DATE[-1])
#str(dates.chr)
values <- cbind(data.r, size, direction)
values <- na.omit(values)
# for dplyr pivoting and ggplot2 need a data frame also known as "tidy data"
data.df <- data.frame(dates = dates, returns = data.r, size = size, direction = direction)
data.df.nd <- data.frame(dates = dates.chr, returns = data.r, size = size, direction = direction, stringsAsFactors = FALSE) 
#non-coerced dates for subsetting on non-date columns
# 2. Make an xts object with row names equal to the dates
data.xts <- na.omit(as.xts(values, dates)) #order.by=as.Date(dates, "%d/%m/%Y")))
#str(data.xts)
data.zr <- as.zooreg(data.xts)
returns <- data.xts # watch for this data below!

# PAGE: Market risk 
corr_rolling <- function(x) {	
  dim <- ncol(x)	
  corr_r <- cor(x)[lower.tri(diag(dim), diag = FALSE)]	
  return(corr_r)	
}
vol_rolling <- function(x){
  library(matrixStats)
  vol_r <- colSds(x)
  return(vol_r)
}
ALL.r <- data.xts[, 1:2]
window <- 90 #reactive({input$window})
corr_r <- rollapply(ALL.r, width = window, corr_rolling, align = "right", by.column = FALSE)
colnames(corr_r) <- c("Coffee.tea")
vol_r <- rollapply(ALL.r, width = window, vol_rolling, align = "right", by.column = FALSE)
colnames(vol_r) <- c("coffee.vol", "tea.vol")
year <- format(index(corr_r), "%Y")
r_corr_vol <- merge(ALL.r, corr_r, vol_r, year)
##
# Load the data_moments() function
## data_moments function
## INPUTS: r vector
## OUTPUTS: list of scalars (mean, sd, median, skewness, kurtosis)
data_moments <- function(data){
  library(moments)
  library(matrixStats)
  mean.r <- colMeans(data)
  median.r <- colMedians(data)
  sd.r <- colSds(data)
  IQR.r <- colIQRs(data)
  skewness.r <- skewness(data)
  kurtosis.r <- kurtosis(data)
  result <- data.frame(mean = mean.r, median = median.r, std_dev = sd.r, IQR = IQR.r, skewness = skewness.r, kurtosis = kurtosis.r)
  return(result)
}
# Run data_moments()
answer <- data_moments(data.xts[, 1:2])
# Build pretty table
answer <- round(answer, 4)
knitr::kable(answer)

```

```{r}

library(quantreg)
taus <- seq(.05,.95,.05)	# Roger Koenker UI Bob Hogg and Allen Craig
fit.rq.Coffee.tea <- rq(log(Coffee.tea) ~ log(coffee.vol), tau = taus, data = r_corr_vol)	
fit.lm.Coffee.tea <- lm(log(Coffee.tea) ~ log(coffee.vol), data = r_corr_vol)	
# Some test statements	
c.t.summary <- summary(fit.rq.Coffee.tea, se = "boot")
plot(c.t.summary)
```
```{r}
title.chg <- "Metals Market Percent Changes"
autoplot.zoo(data.xts[,1:2]) + ggtitle(title.chg) + ylim(-5, 5)
autoplot.zoo(data.xts[,3:4]) + ggtitle(title.chg) + ylim(-5, 5)
acf(coredata(data.xts[,1:2])) # returns
acf(coredata(data.xts[,3:4])) # sizes
#pacf here
one <- ts(data.df$returns.PCOFFROBUSDM)
two <- ts(data.df$returns.PTEAUSDM)
# or
one <- ts(data.zr[,1])
two <- ts(data.zr[,2])
title.chg <- "Coffee vs. Tea"
ccf(one, two, main = title.chg, lag.max = 20, xlab = "", ylab = "", ci.col = "red")

# build function to repeat these routines
run_ccf <- function(one, two, main = title.chg, lag = 20, color = "red"){
  # one and two are equal length series
  # main is title
  # lag is number of lags in cross-correlation
  # color is color of dashed confidence interval bounds
  stopifnot(length(one) == length(two))
  one <- ts(one)
  two <- ts(two)
  main <- main
  lag <- lag
  color <- color
  ccf(one, two, main = main, lag.max = lag, xlab = "", ylab = "", ci.col = color)
  #end run_ccf
}
title <- "coffee-tea"
run_ccf(one, two, main = title, lag = 20, color = "red")
# now for volatility (sizes)
one <- abs(data.zr[,1])
two <- abs(data.zr[,2])
title <- "Coffee-Tea: volatility"
run_ccf(one, two, main = title, lag = 20, color = "red")
```

```{r}
#mean(data.xts[,4])
##
returns1 <- returns[,1]
colnames(returns1) <- "Returns" #kluge to coerce column name for df
returns1.df <- data.frame(Returns = returns1[,1], Distribution = rep("Historical", each = length(returns1)))
  
alpha <- 0.95 # reactive({ifelse(input$alpha.q>1,0.99,ifelse(input$alpha.q<0,0.001,input$alpha.q))})
  
# Value at Risk
VaR.hist <- quantile(returns1,alpha)
VaR.text <- paste("Value at Risk =", round(VaR.hist, 2))
  
# Determine the max y value of the desity plot.
# This will be used to place the text above the plot
VaR.y <- max(density(returns1.df$Returns)$y)
  
# Expected Shortfall
ES.hist <- median(returns1[returns1 > VaR.hist])
ES.text <- paste("Expected Shortfall =", round(ES.hist, 2))
  
p <- ggplot(returns1.df, aes(x = Returns, fill = Distribution)) + geom_density(alpha = 0.5) + 
    geom_vline(aes(xintercept = VaR.hist), linetype = "dashed", size = 1, color = "firebrick1") + 
    geom_vline(aes(xintercept = ES.hist), size = 1, color = "firebrick1") +
    annotate("text", x = 2+ VaR.hist, y = VaR.y*1.05, label = VaR.text) +
    annotate("text", x = 1.5+ ES.hist, y = VaR.y*1.1, label = ES.text) + scale_fill_manual( values = "dodgerblue4")
p
```
```{r}

# Do the same for returns 2 aand 3
##
## Now for Loss Analysis
# Get last prices
price.last <- as.numeric(head(data[, -1], n=1))
# Specify the positions
position.rf <- c(1, 1, 1)
# And compute the position weights
w <- position.rf * price.last
# Fan these  the length and breadth of the risk factor series
weights.rf <- matrix(w, nrow=nrow(data.r), ncol=ncol(data.r), byrow=TRUE)
#head(rowSums((exp(data.r/100)-1)*weights.rf), n=3)
## We need to compute exp(x) - 1 for very small x: expm1 accomplishes this
#head(rowSums((exp(data.r/100)-1)*weights.rf), n=4)
loss.rf <- -rowSums(expm1(data.r/100) * weights.rf)
loss.rf.df <- data.frame(Loss = loss.rf, Distribution = rep("Historical", each = length(loss.rf)))
## Simple Value at Risk and Expected Shortfall
alpha.tolerance <- .95
VaR.hist <- quantile(loss.rf, probs=alpha.tolerance, names=FALSE)
## Just as simple Expected shortfall
ES.hist <- median(loss.rf[loss.rf > VaR.hist])
VaR.text <- paste("Value at Risk =\n", round(VaR.hist, 2)) # ="VaR"&c12
ES.text <- paste("Expected Shortfall \n=", round(ES.hist, 2))
title.text <- paste(round(alpha.tolerance*100, 0), "% Loss Limits")
# using histogram bars instead of the smooth density
p <- ggplot(loss.rf.df, aes(x = Loss, fill = Distribution)) + geom_histogram(alpha = 0.8) + geom_vline(aes(xintercept = VaR.hist), linetype = "dashed", size = 1, color = "blue") + geom_vline(aes(xintercept = ES.hist), size = 1, color = "blue") + annotate("text", x = VaR.hist, y = 40, label = VaR.text) + annotate("text", x = ES.hist, y = 20, label = ES.text) + xlim(0, 500) + ggtitle(title.text)
p


```

```{r}
# mean excess plot to determine thresholds for extreme event management
data <- as.vector(loss.rf) # data is purely numeric
umin <-  min(data)         # threshold u min
umax <-  max(data) - 0.1   # threshold u max
nint <- 100                # grid length to generate mean excess plot
grid.0 <- numeric(nint)    # grid store
e <- grid.0                # store mean exceedances e
upper <- grid.0            # store upper confidence interval
lower <- grid.0            # store lower confidence interval
u <- seq(umin, umax, length = nint) # threshold u grid
alpha <- 0.95                  # confidence level
for (i in 1:nint) {
    data <- data[data > u[i]]  # subset data above thresholds
    e[i] <- mean(data - u[i])  # calculate mean excess of threshold
    sdev <- sqrt(var(data))    # standard deviation
    n <- length(data)          # sample size of subsetted data above thresholds
    upper[i] <- e[i] + (qnorm((1 + alpha)/2) * sdev)/sqrt(n) # upper confidence interval
    lower[i] <- e[i] - (qnorm((1 + alpha)/2) * sdev)/sqrt(n) # lower confidence interval
  }
mep.df <- data.frame(threshold = u, threshold.exceedances = e, lower = lower, upper = upper)
loss.excess <- loss.rf[loss.rf > u]
# Voila the plot => you may need to tweak these limits!
plt <- ggplot(mep.df, aes( x= threshold, y = threshold.exceedances)) + geom_line() + geom_line(aes(x = threshold, y = lower), colour = "red") + geom_line(aes(x = threshold,  y = upper), colour = "red") + annotate("text", x = 75, y = 125, label = "upper 95%") + annotate("text", x = 75, y = 0, label = "lower 5%")
plt
```
```{r}

```

# ```{r}
# library(configr)
# keys <- read.config(file = Sys.getenv("R_CONFIGFILE_ACTIVE", "config.cfg"),
#   extra.list = list(), other.config = "", rcmd.parse = FALSE,
#   bash.parse = FALSE, glue.parse = FALSE, glue.flag = "!!glue",
#   global.vars.field = "global_vars", file.type = NULL)
# ```
# 
# ```{r}
# 
# Sys.setenv("plotly_username"=keys$Plotly$user)
# Sys.setenv("plotly_api_key"=keys$Plotly$key)
# ```
# 
# ```{r}
# api_create(p, filename = "getting-started/ggplotly")
# ```

